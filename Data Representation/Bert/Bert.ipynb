{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T11:01:37.657994Z",
     "start_time": "2025-06-23T11:00:40.829067Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from pymongo import MongoClient\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ BERT\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()  # ØªØ¹Ø·ÙŠÙ„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨\n",
    "\n",
    "# CUDA Ø¥Ø°Ø§ Ù…ØªØ§Ø­\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def get_bert_embedding(text):\n",
    "    \"\"\"ØªØ­ÙˆÙŠÙ„ Ù†Øµ Ø¥Ù„Ù‰ ØªÙ…Ø«ÙŠÙ„ BERT\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "    return embeddings.squeeze().cpu().numpy()\n",
    "\n",
    "def process_bert_embedding(dataset_name, collection_name):\n",
    "    print(f\"ğŸš€ Processing BERT embeddings from MongoDB collection: {collection_name}...\")\n",
    "\n",
    "    # Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "    client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "    db = client[\"ir_project\"]\n",
    "    collection = db[collection_name]\n",
    "\n",
    "    # Ø¬Ù„Ø¨ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚\n",
    "    documents = list(collection.find({}, {\"_id\": 0, \"doc_id\": 1, \"original_text\": 1}))\n",
    "    documents = [doc for doc in documents if doc.get(\"original_text\", \"\").strip()]\n",
    "\n",
    "    if not documents:\n",
    "        print(\"âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØµÙˆØµ Ø£ØµÙ„ÙŠØ© Ù…ØªØ§Ø­Ø© Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©.\")\n",
    "        return\n",
    "\n",
    "    doc_ids = [doc[\"doc_id\"] for doc in documents]\n",
    "    texts = [doc[\"original_text\"] for doc in documents]\n",
    "\n",
    "    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØªÙ…Ø«ÙŠÙ„Ø§Øª\n",
    "    all_embeddings = []\n",
    "    for text in tqdm(texts, desc=\"Embedding\"):\n",
    "        emb = get_bert_embedding(text)\n",
    "        all_embeddings.append(emb)\n",
    "\n",
    "    # Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "    embedding_data = {\n",
    "        \"doc_ids\": doc_ids,\n",
    "        \"embeddings_matrix\": all_embeddings,\n",
    "        \"model_name\": MODEL_NAME\n",
    "    }\n",
    "\n",
    "    save_path = os.path.join(dataset_name.replace(\"/\", os.sep), \"doc\")\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    joblib.dump(embedding_data, os.path.join(save_path, \"bert_embedding.joblib\"))\n",
    "\n",
    "    print(f\"âœ… BERT embeddings saved to: {os.path.join(save_path, 'bert_embedding.joblib')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e05557dbfb5294e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T16:52:58.153754Z",
     "start_time": "2025-06-23T14:18:31.569897Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Azzam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Azzam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Azzam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Azzam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Loading and processing dataset: antique/train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Processing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 403666/403666 [2:30:56<00:00, 44.57it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… BERT embeddings saved to: antique\\train\\doc\\bert_embedding.joblib\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import string\n",
    "import torch\n",
    "import joblib\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import ir_datasets\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "punctuations = set(string.punctuation)\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ BERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Ø¯Ø§Ù„Ø© ØªÙ†Ø¸ÙŠÙ\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    cleaned_tokens = []\n",
    "    for token in tokens:\n",
    "        if token in punctuations or token in stop_words or token.isdigit():\n",
    "            continue\n",
    "        lemma = lemmatizer.lemmatize(token)\n",
    "        if len(lemma) < 3:\n",
    "            continue\n",
    "        cleaned_tokens.append(lemma)\n",
    "    return ' '.join(cleaned_tokens)\n",
    "\n",
    "# Ø¯Ø§Ù„Ø© ØªÙ…Ø«ÙŠÙ„ BERT\n",
    "def get_bert_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "    return embeddings.squeeze().cpu().numpy()\n",
    "\n",
    "# Ø§Ù„Ù…Ø³Ø§Ø± ÙˆØ§Ù„Ø¯Ø§ØªØ§Ø³ÙŠØª\n",
    "dataset_name = \"antique/train\"\n",
    "dataset = ir_datasets.load(dataset_name)\n",
    "save_path = os.path.join(dataset_name.replace(\"/\", os.sep), \"doc\")\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "save_file = os.path.join(save_path, \"bert_embedding.joblib\")\n",
    "\n",
    "print(f\"ğŸš€ Loading and processing dataset: {dataset_name}\")\n",
    "\n",
    "doc_ids = []\n",
    "embeddings = []\n",
    "\n",
    "for doc in tqdm(dataset.docs_iter(), total=dataset.docs_count(), desc=\"ğŸ”„ Processing\"):\n",
    "    clean_text = preprocess(doc.text)\n",
    "    if not clean_text.strip():\n",
    "        continue\n",
    "    emb = get_bert_embedding(clean_text)\n",
    "    doc_ids.append(doc.doc_id)\n",
    "    embeddings.append(emb)\n",
    "\n",
    "# Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "embedding_data = {\n",
    "    \"doc_ids\": doc_ids,\n",
    "    \"embeddings_matrix\": embeddings,\n",
    "    \"model_name\": MODEL_NAME\n",
    "}\n",
    "joblib.dump(embedding_data, save_file)\n",
    "\n",
    "print(f\"âœ… BERT embeddings saved to: {save_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9fb43223f756cea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T20:26:24.632498Z",
     "start_time": "2025-06-18T18:24:09.800340Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Processing BERT embeddings from MongoDB collection: documents_quora_test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 522931/522931 [1:59:47<00:00, 72.76it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… BERT embeddings saved to: beir\\quora\\test\\doc\\bert_embedding.joblib\n"
     ]
    }
   ],
   "source": [
    "# Quora Dataset\n",
    "process_bert_embedding(\"beir/quora/test\", \"documents_quora_test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
