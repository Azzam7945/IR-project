{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T11:01:37.657994Z",
     "start_time": "2025-06-23T11:00:40.829067Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from pymongo import MongoClient\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# تحميل BERT\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()  # تعطيل التدريب\n",
    "\n",
    "# CUDA إذا متاح\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def get_bert_embedding(text):\n",
    "    \"\"\"تحويل نص إلى تمثيل BERT\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "    return embeddings.squeeze().cpu().numpy()\n",
    "\n",
    "def process_bert_embedding(dataset_name, collection_name):\n",
    "    print(f\"🚀 Processing BERT embeddings from MongoDB collection: {collection_name}...\")\n",
    "\n",
    "    # الاتصال بقاعدة البيانات\n",
    "    client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "    db = client[\"ir_project\"]\n",
    "    collection = db[collection_name]\n",
    "\n",
    "    # جلب الوثائق\n",
    "    documents = list(collection.find({}, {\"_id\": 0, \"doc_id\": 1, \"original_text\": 1}))\n",
    "    documents = [doc for doc in documents if doc.get(\"original_text\", \"\").strip()]\n",
    "\n",
    "    if not documents:\n",
    "        print(\"❌ لا توجد نصوص أصلية متاحة للمعالجة.\")\n",
    "        return\n",
    "\n",
    "    doc_ids = [doc[\"doc_id\"] for doc in documents]\n",
    "    texts = [doc[\"original_text\"] for doc in documents]\n",
    "\n",
    "    # الحصول على التمثيلات\n",
    "    all_embeddings = []\n",
    "    for text in tqdm(texts, desc=\"Embedding\"):\n",
    "        emb = get_bert_embedding(text)\n",
    "        all_embeddings.append(emb)\n",
    "\n",
    "    # حفظ البيانات\n",
    "    embedding_data = {\n",
    "        \"doc_ids\": doc_ids,\n",
    "        \"embeddings_matrix\": all_embeddings,\n",
    "        \"model_name\": MODEL_NAME\n",
    "    }\n",
    "\n",
    "    save_path = os.path.join(dataset_name.replace(\"/\", os.sep), \"doc\")\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    joblib.dump(embedding_data, os.path.join(save_path, \"bert_embedding.joblib\"))\n",
    "\n",
    "    print(f\"✅ BERT embeddings saved to: {os.path.join(save_path, 'bert_embedding.joblib')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e05557dbfb5294e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T16:52:58.153754Z",
     "start_time": "2025-06-23T14:18:31.569897Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Azzam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Azzam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Azzam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Azzam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Loading and processing dataset: antique/train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔄 Processing: 100%|██████████| 403666/403666 [2:30:56<00:00, 44.57it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BERT embeddings saved to: antique\\train\\doc\\bert_embedding.joblib\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import string\n",
    "import torch\n",
    "import joblib\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import ir_datasets\n",
    "\n",
    "# تحميل الموارد\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# الإعدادات\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "punctuations = set(string.punctuation)\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# تحميل BERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# دالة تنظيف\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    cleaned_tokens = []\n",
    "    for token in tokens:\n",
    "        if token in punctuations or token in stop_words or token.isdigit():\n",
    "            continue\n",
    "        lemma = lemmatizer.lemmatize(token)\n",
    "        if len(lemma) < 3:\n",
    "            continue\n",
    "        cleaned_tokens.append(lemma)\n",
    "    return ' '.join(cleaned_tokens)\n",
    "\n",
    "# دالة تمثيل BERT\n",
    "def get_bert_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "    return embeddings.squeeze().cpu().numpy()\n",
    "\n",
    "# المسار والداتاسيت\n",
    "dataset_name = \"antique/train\"\n",
    "dataset = ir_datasets.load(dataset_name)\n",
    "save_path = os.path.join(dataset_name.replace(\"/\", os.sep), \"doc\")\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "save_file = os.path.join(save_path, \"bert_embedding.joblib\")\n",
    "\n",
    "print(f\"🚀 Loading and processing dataset: {dataset_name}\")\n",
    "\n",
    "doc_ids = []\n",
    "embeddings = []\n",
    "\n",
    "for doc in tqdm(dataset.docs_iter(), total=dataset.docs_count(), desc=\"🔄 Processing\"):\n",
    "    clean_text = preprocess(doc.text)\n",
    "    if not clean_text.strip():\n",
    "        continue\n",
    "    emb = get_bert_embedding(clean_text)\n",
    "    doc_ids.append(doc.doc_id)\n",
    "    embeddings.append(emb)\n",
    "\n",
    "# حفظ البيانات\n",
    "embedding_data = {\n",
    "    \"doc_ids\": doc_ids,\n",
    "    \"embeddings_matrix\": embeddings,\n",
    "    \"model_name\": MODEL_NAME\n",
    "}\n",
    "joblib.dump(embedding_data, save_file)\n",
    "\n",
    "print(f\"✅ BERT embeddings saved to: {save_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9fb43223f756cea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T20:26:24.632498Z",
     "start_time": "2025-06-18T18:24:09.800340Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing BERT embeddings from MongoDB collection: documents_quora_test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 522931/522931 [1:59:47<00:00, 72.76it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BERT embeddings saved to: beir\\quora\\test\\doc\\bert_embedding.joblib\n"
     ]
    }
   ],
   "source": [
    "# Quora Dataset\n",
    "process_bert_embedding(\"beir/quora/test\", \"documents_quora_test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
