{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-24T08:55:03.495528Z",
     "start_time": "2025-06-24T08:54:50.578600Z"
    }
   },
   "source": [
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def tokenize(text):\n",
    "    # تقسيم النص إلى كلمات بسيطة (يمكن تحسينها لاحقًا بإزالة علامات الترقيم والكلمات التوقف)\n",
    "    text = text.lower()\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "    return tokens\n",
    "\n",
    "def build_inverted_index(docs):\n",
    "    inverted_index = defaultdict(set)  # الكلمة -> مجموعة معرفات المستندات\n",
    "\n",
    "    for doc in docs:\n",
    "        doc_id = doc[\"doc_id\"]\n",
    "        text = doc[\"clean_text\"]\n",
    "        tokens = tokenize(text)\n",
    "        unique_tokens = set(tokens)\n",
    "        for token in unique_tokens:\n",
    "            inverted_index[token].add(doc_id)\n",
    "\n",
    "    # تحويل المجموعات لقوائم (لتسهيل التخزين أو المعالجة)\n",
    "    inverted_index = {term: list(doc_ids) for term, doc_ids in inverted_index.items()}\n",
    "    return inverted_index\n",
    "\n",
    "def main():\n",
    "    # قراءة الملف JSON\n",
    "    path = r\"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\Data Pre-Processing\\beir\\quora\\test\\doc\\docs.json\"\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        docs = json.load(f)\n",
    "\n",
    "    print(f\"عدد المستندات: {len(docs)}\")\n",
    "\n",
    "    # بناء الفهرس\n",
    "    inverted_index = build_inverted_index(docs)\n",
    "\n",
    "    # مثال: عرض بعض المصطلحات وعدد مستنداتها\n",
    "    for term in list(inverted_index.keys())[:10]:\n",
    "        print(f\"'{term}': {len(inverted_index[term])} مستندات\")\n",
    "\n",
    "    # حفظ الفهرس إلى ملف JSON\n",
    "    with open('beir_inverted_index.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(inverted_index, f, ensure_ascii=False, indent=2)\n",
    "    print(\"✅ تم حفظ الفهرس في beir_inverted_index.json\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "عدد المستندات: 522931\n",
      "'step': 898 مستندات\n",
      "'guide': 291 مستندات\n",
      "'india': 17246 مستندات\n",
      "'invest': 1105 مستندات\n",
      "'market': 1906 مستندات\n",
      "'share': 1171 مستندات\n",
      "'i': 760 مستندات\n",
      "'koh': 15 مستندات\n",
      "'noor': 5 مستندات\n",
      "'kohinoor': 12 مستندات\n",
      "✅ تم حفظ الفهرس في beir_inverted_index.json\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T13:43:40.524532Z",
     "start_time": "2025-06-19T13:43:24.229662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "chunks_path = r\"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\Data Representation\\Hybrid\\beir\\quora\\test\\chunks\"\n",
    "docid_to_chunk = {}\n",
    "\n",
    "for chunk_file in os.listdir(chunks_path):\n",
    "    if not chunk_file.startswith(\"hybrid_chunk_\") or not chunk_file.endswith(\".joblib\"):\n",
    "        continue\n",
    "    chunk_path = os.path.join(chunks_path, chunk_file)\n",
    "    chunk_data = joblib.load(chunk_path)\n",
    "    for doc_id in chunk_data[\"doc_ids\"]:\n",
    "        docid_to_chunk[doc_id] = chunk_file\n",
    "\n",
    "# حفظ الخريطة\n",
    "with open(os.path.join(chunks_path, \"docid_to_chunk.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(docid_to_chunk, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ تم بناء وحفظ خريطة doc_id إلى chunk_file في {chunks_path}\\\\docid_to_chunk.json\")\n"
   ],
   "id": "8eaeb744b772c629",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ تم بناء وحفظ خريطة doc_id إلى chunk_file في C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\Data Representation\\Hybrid\\beir\\quora\\test\\chunks\\docid_to_chunk.json\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T13:48:53.458350Z",
     "start_time": "2025-06-19T13:48:49.033472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import vstack\n",
    "\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "    return tokens\n",
    "\n",
    "# تحميل الفهرس العكسي\n",
    "with open('inverted_index.json', 'r', encoding='utf-8') as f:\n",
    "    inverted_index = json.load(f)\n",
    "\n",
    "# تحميل موديل BERT\n",
    "bert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def get_candidate_doc_ids(query_tokens):\n",
    "    candidate_sets = []\n",
    "    for token in query_tokens:\n",
    "        if token in inverted_index:\n",
    "            candidate_sets.append(set(inverted_index[token]))\n",
    "    if candidate_sets:\n",
    "        # اتحاد مجموعات المستندات التي تحتوي على أي من الكلمات\n",
    "        candidates = set.union(*candidate_sets)\n",
    "    else:\n",
    "        candidates = set()\n",
    "    return candidates\n",
    "\n",
    "def load_chunks_for_docs_with_map(candidates, chunks_path, docid_to_chunk_path):\n",
    "    # تحميل خريطة doc_id -> chunk_file\n",
    "    with open(docid_to_chunk_path, 'r', encoding='utf-8') as f:\n",
    "        docid_to_chunk = json.load(f)\n",
    "\n",
    "    # خزن لكل chunk_file قائمة doc_ids المطلوبة\n",
    "    chunk_to_docids = {}\n",
    "\n",
    "    for doc_id in candidates:\n",
    "        chunk_file = docid_to_chunk.get(doc_id)\n",
    "        if chunk_file is not None:\n",
    "            chunk_to_docids.setdefault(chunk_file, []).append(doc_id)\n",
    "\n",
    "    tfidf_chunks = []\n",
    "    bert_chunks = []\n",
    "    doc_ids_all = []\n",
    "\n",
    "    for chunk_file, doc_ids_needed in chunk_to_docids.items():\n",
    "        chunk_path = os.path.join(chunks_path, chunk_file)\n",
    "        chunk_data = joblib.load(chunk_path)\n",
    "        chunk_doc_ids = chunk_data[\"doc_ids\"]\n",
    "\n",
    "        # جلب الإندكس للـ doc_ids المطلوبة ضمن هذا الـ chunk\n",
    "        indices = [i for i, doc_id in enumerate(chunk_doc_ids) if doc_id in doc_ids_needed]\n",
    "\n",
    "        if not indices:\n",
    "            continue\n",
    "\n",
    "        tfidf_chunk = chunk_data[\"tfidf_chunk\"][indices]\n",
    "        bert_chunk = chunk_data[\"bert_chunk\"][indices]\n",
    "        selected_doc_ids = [chunk_doc_ids[i] for i in indices]\n",
    "\n",
    "        tfidf_chunks.append(tfidf_chunk)\n",
    "        bert_chunks.append(bert_chunk)\n",
    "        doc_ids_all.extend(selected_doc_ids)\n",
    "\n",
    "    if tfidf_chunks:\n",
    "        tfidf_matrix = vstack(tfidf_chunks)\n",
    "        bert_matrix = np.vstack(bert_chunks)\n",
    "    else:\n",
    "        tfidf_matrix = None\n",
    "        bert_matrix = None\n",
    "\n",
    "    return tfidf_matrix, bert_matrix, doc_ids_all\n",
    "\n",
    "def hybrid_search(query, vectorizer, chunks_path, docid_to_chunk_path, tfidf_weight=0.5, bert_weight=0.5, top_k=10):\n",
    "    query_tokens = tokenize(query)\n",
    "\n",
    "    candidates = get_candidate_doc_ids(query_tokens)\n",
    "    if not candidates:\n",
    "        print(\"⚠️ لا توجد مستندات مرشحة للفحص.\")\n",
    "        return []\n",
    "\n",
    "    tfidf_matrix, bert_matrix, doc_ids = load_chunks_for_docs_with_map(candidates, chunks_path, docid_to_chunk_path)\n",
    "\n",
    "    if tfidf_matrix is None or bert_matrix is None:\n",
    "        print(\"⚠️ لم يتم تحميل تمثيلات لأي مستندات.\")\n",
    "        return []\n",
    "\n",
    "    query_tfidf = vectorizer.transform([query])\n",
    "    query_bert = bert_model.encode(query, convert_to_numpy=True).reshape(1, -1).astype(np.float32)\n",
    "\n",
    "    tfidf_sim = cosine_similarity(tfidf_matrix, query_tfidf).flatten()\n",
    "    bert_sim = cosine_similarity(bert_matrix, query_bert).flatten()\n",
    "\n",
    "    final_scores = tfidf_weight * tfidf_sim + bert_weight * bert_sim\n",
    "\n",
    "    top_indices = np.argsort(final_scores)[::-1][:top_k]\n",
    "    results = [(doc_ids[i], float(final_scores[i])) for i in top_indices]\n",
    "\n",
    "    return results\n",
    "\n",
    "# --- استخدام الدالة ---\n",
    "\n",
    "\n"
   ],
   "id": "fdfcc8ed68a58405",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T13:53:06.408483Z",
     "start_time": "2025-06-19T13:52:48.404337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "\n",
    "# 1. استيراد الدالة اللي كتبناها (إذا في ملف منفصل)\n",
    "# من فرضياتنا، الدالة hybrid_search موجودة هنا في نفس السكربت\n",
    "\n",
    "# 2. تحميل vectorizer من ملف tfidf_data.joblib (في عندك الملف)\n",
    "vectorizer_data = joblib.load(r\"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\Data Representation\\TF-IDF\\beir\\quora\\test\\doc\\tfidf_data.joblib\")\n",
    "vectorizer = vectorizer_data[\"vectorizer\"]\n",
    "\n",
    "# 3. تحديد مسار ملفات التمثيل الهجين (chunks)\n",
    "chunks_path = r\"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\Data Representation\\Hybrid\\beir\\quora\\test\\chunks\"\n",
    "docid_to_chunk_path = os.path.join(chunks_path, \"docid_to_chunk.json\")\n",
    "\n",
    "# 4. الاستعلام اللي بدنا نبحث فيه\n",
    "query = \"how to learn a amchine programming language?\"\n",
    "\n",
    "# 5. تنفيذ البحث الهجين\n",
    "results = hybrid_search(\n",
    "    query=query,\n",
    "    vectorizer=vectorizer,\n",
    "    chunks_path=chunks_path,\n",
    "     docid_to_chunk_path=docid_to_chunk_path,\n",
    "    tfidf_weight=0.4,\n",
    "    bert_weight=0.6,\n",
    "    top_k=10\n",
    ")\n",
    "\n",
    "# 6. طباعة النتائج\n",
    "print(\"✅ نتائج البحث النهائية:\")\n",
    "for doc_id, score in results:\n",
    "    print(f\"Doc ID: {doc_id} | Score: {score:.4f}\")\n"
   ],
   "id": "583ef323cee71452",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ نتائج البحث النهائية:\n",
      "Doc ID: 91642 | Score: 0.6005\n",
      "Doc ID: 527808 | Score: 0.5991\n",
      "Doc ID: 498101 | Score: 0.5965\n",
      "Doc ID: 44161 | Score: 0.5810\n",
      "Doc ID: 1271 | Score: 0.5602\n",
      "Doc ID: 112819 | Score: 0.5481\n",
      "Doc ID: 364397 | Score: 0.5396\n",
      "Doc ID: 9811 | Score: 0.5242\n",
      "Doc ID: 22650 | Score: 0.5242\n",
      "Doc ID: 419330 | Score: 0.5234\n"
     ]
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
