{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-03T09:39:00.122528Z",
     "start_time": "2025-07-03T09:39:00.108980Z"
    }
   },
   "source": [
    "import os\n",
    "import joblib\n",
    "import ir_datasets\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from difflib import get_close_matches\n",
    "\n",
    "# ==========================\n",
    "# ğŸ§  QueryRefiner - ØªØµØ­ÙŠØ­ ÙÙ‚Ø·\n",
    "# ==========================\n",
    "class QueryRefinerCorrectOnly:\n",
    "    def __init__(self, processed_terms):\n",
    "        self.term_frequencies = Counter(processed_terms)\n",
    "        self.processed_terms = set(processed_terms)\n",
    "\n",
    "    def suggest_correction(self, query):\n",
    "        words = query.split()\n",
    "        corrected = []\n",
    "        for word in words:\n",
    "            matches = get_close_matches(word, self.processed_terms, n=1, cutoff=0.8)\n",
    "            if matches:\n",
    "                corrected.append(matches[0])\n",
    "            else:\n",
    "                corrected.append(word)\n",
    "        return corrected\n",
    "\n",
    "    def enhance(self, query):\n",
    "        corrected = self.suggest_correction(query)\n",
    "        return \" \".join(corrected)\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# ğŸ”„ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª\n",
    "# ==========================\n",
    "def generate_corrected_queries(dataset_name, terms_path, output_path, batch_size=1000):\n",
    "    print(f\"ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ù…Ù†: {dataset_name}\")\n",
    "    dataset = ir_datasets.load(dataset_name)\n",
    "    queries = list(dataset.queries_iter())\n",
    "\n",
    "    print(f\"ğŸ“– ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª Ù…Ù†: {terms_path}\")\n",
    "    with open(terms_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        terms = [line.strip().lower() for line in f if line.strip()]\n",
    "    refiner = QueryRefinerCorrectOnly(terms)\n",
    "\n",
    "    enhanced_queries = {}\n",
    "\n",
    "    print(\"ğŸ”§ ØªØµØ­ÙŠØ­ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª ÙÙ‚Ø·...\")\n",
    "    for i in range(0, len(queries), batch_size):\n",
    "        batch = queries[i:i+batch_size]\n",
    "        for q in tqdm(batch, desc=f\"ğŸ”¤ Ø¯ÙØ¹Ø© {i//batch_size+1}\"):\n",
    "            enhanced = refiner.enhance(q.text)\n",
    "            enhanced_queries[q.query_id] = enhanced\n",
    "\n",
    "        # Ø­ÙØ¸ Ù…Ø¤Ù‚Øª Ù„ÙƒÙ„ Ø¯ÙØ¹Ø©\n",
    "        partial_path = output_path.replace(\".joblib\", f\"_corrected_part{i//batch_size+1}.joblib\")\n",
    "        os.makedirs(os.path.dirname(partial_path), exist_ok=True)\n",
    "        joblib.dump(enhanced_queries, partial_path)\n",
    "        print(f\"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ø¯ÙØ¹Ø© {i//batch_size+1} ÙÙŠ: {partial_path}\")\n",
    "\n",
    "    joblib.dump(enhanced_queries, output_path)\n",
    "    print(f\"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ù…ØµØ­Ø­Ø© ÙÙŠ: {output_path}\")\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T09:41:39.119968Z",
     "start_time": "2025-07-03T09:39:15.522421Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==========================\n",
    "# â–¶ï¸ ØªÙ†ÙÙŠØ° Ø§Ù„Ø³ÙƒØ±Ø¨Øª\n",
    "# ==========================\n",
    "if __name__ == \"__main__\":\n",
    "    # ğŸ”§ Ø¹Ø¯Ù‘Ù„ Ù‡Ø°Ù‡ Ø§Ù„Ù‚ÙŠÙ… Ø­Ø³Ø¨ Ø§Ù„Ø­Ø§Ø¬Ø©\n",
    "    dataset_name = \"beir/quora/test\"\n",
    "    terms_path = r\"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\vocabularies\\beir_quora_test_terms.txt\"\n",
    "    output_path = r\"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\vocabularies\\Evaluation Query\\enhanced_queries.joblib\"\n",
    "    batch_size = 100\n",
    "\n",
    "    generate_corrected_queries(\n",
    "        dataset_name=dataset_name,\n",
    "        terms_path=terms_path,\n",
    "        output_path=output_path,\n",
    "        batch_size=batch_size\n",
    "    )\n"
   ],
   "id": "cb7aba01e728cf7d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ù…Ù†: beir/quora/test\n",
      "ğŸ“– ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª Ù…Ù†: C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\vocabularies\\beir_quora_test_terms.txt\n",
      "ğŸ”§ ØªØµØ­ÙŠØ­ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª ÙÙ‚Ø·...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ”¤ Ø¯ÙØ¹Ø© 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [01:51<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ø¯ÙØ¹Ø© 1 ÙÙŠ: C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\vocabularies\\Evaluation Query\\enhanced_queries_corrected_part1.joblib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ”¤ Ø¯ÙØ¹Ø© 2:  30%|â–ˆâ–ˆâ–ˆ       | 30/100 [00:31<01:12,  1.04s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 11\u001B[39m\n\u001B[32m      8\u001B[39m output_path = \u001B[33mr\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mC:\u001B[39m\u001B[33m\\\u001B[39m\u001B[33mUsers\u001B[39m\u001B[33m\\\u001B[39m\u001B[33mAzzam\u001B[39m\u001B[33m\\\u001B[39m\u001B[33mPycharmProjects\u001B[39m\u001B[33m\\\u001B[39m\u001B[33mPythonProject\u001B[39m\u001B[33m\\\u001B[39m\u001B[33mvocabularies\u001B[39m\u001B[33m\\\u001B[39m\u001B[33mEvaluation Query\u001B[39m\u001B[33m\\\u001B[39m\u001B[33menhanced_queries.joblib\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m      9\u001B[39m batch_size = \u001B[32m100\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m11\u001B[39m \u001B[43mgenerate_corrected_queries\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     12\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdataset_name\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdataset_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     13\u001B[39m \u001B[43m    \u001B[49m\u001B[43mterms_path\u001B[49m\u001B[43m=\u001B[49m\u001B[43mterms_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     14\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_path\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     15\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbatch_size\u001B[49m\n\u001B[32m     16\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 51\u001B[39m, in \u001B[36mgenerate_corrected_queries\u001B[39m\u001B[34m(dataset_name, terms_path, output_path, batch_size)\u001B[39m\n\u001B[32m     49\u001B[39m batch = queries[i:i+batch_size]\n\u001B[32m     50\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m q \u001B[38;5;129;01min\u001B[39;00m tqdm(batch, desc=\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mğŸ”¤ Ø¯ÙØ¹Ø© \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi//batch_size+\u001B[32m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m):\n\u001B[32m---> \u001B[39m\u001B[32m51\u001B[39m     enhanced = \u001B[43mrefiner\u001B[49m\u001B[43m.\u001B[49m\u001B[43menhance\u001B[49m\u001B[43m(\u001B[49m\u001B[43mq\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     52\u001B[39m     enhanced_queries[q.query_id] = enhanced\n\u001B[32m     54\u001B[39m \u001B[38;5;66;03m# Ø­ÙØ¸ Ù…Ø¤Ù‚Øª Ù„ÙƒÙ„ Ø¯ÙØ¹Ø©\u001B[39;00m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 28\u001B[39m, in \u001B[36mQueryRefinerCorrectOnly.enhance\u001B[39m\u001B[34m(self, query)\u001B[39m\n\u001B[32m     27\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34menhance\u001B[39m(\u001B[38;5;28mself\u001B[39m, query):\n\u001B[32m---> \u001B[39m\u001B[32m28\u001B[39m     corrected = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msuggest_correction\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     29\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33m \u001B[39m\u001B[33m\"\u001B[39m.join(corrected)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 20\u001B[39m, in \u001B[36mQueryRefinerCorrectOnly.suggest_correction\u001B[39m\u001B[34m(self, query)\u001B[39m\n\u001B[32m     18\u001B[39m corrected = []\n\u001B[32m     19\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m words:\n\u001B[32m---> \u001B[39m\u001B[32m20\u001B[39m     matches = \u001B[43mget_close_matches\u001B[49m\u001B[43m(\u001B[49m\u001B[43mword\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mprocessed_terms\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcutoff\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.8\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     21\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m matches:\n\u001B[32m     22\u001B[39m         corrected.append(matches[\u001B[32m0\u001B[39m])\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\difflib.py:705\u001B[39m, in \u001B[36mget_close_matches\u001B[39m\u001B[34m(word, possibilities, n, cutoff)\u001B[39m\n\u001B[32m    702\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m possibilities:\n\u001B[32m    703\u001B[39m     s.set_seq1(x)\n\u001B[32m    704\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m s.real_quick_ratio() >= cutoff \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[32m--> \u001B[39m\u001B[32m705\u001B[39m        \u001B[43ms\u001B[49m\u001B[43m.\u001B[49m\u001B[43mquick_ratio\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m >= cutoff \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[32m    706\u001B[39m        s.ratio() >= cutoff:\n\u001B[32m    707\u001B[39m         result.append((s.ratio(), x))\n\u001B[32m    709\u001B[39m \u001B[38;5;66;03m# Move the best scorers to head of list\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\difflib.py:-1\u001B[39m, in \u001B[36mSequenceMatcher.quick_ratio\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m      0\u001B[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T12:39:12.004839Z",
     "start_time": "2025-07-01T12:08:27.350281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==========================\n",
    "# â–¶ï¸ ØªÙ†ÙÙŠØ° Ø§Ù„Ø³ÙƒØ±Ø¨Øª\n",
    "# ==========================\n",
    "if __name__ == \"__main__\":\n",
    "    # ğŸ”§ Ø¹Ø¯Ù‘Ù„ Ù‡Ø°Ù‡ Ø§Ù„Ù‚ÙŠÙ… Ø­Ø³Ø¨ Ø§Ù„Ø­Ø§Ø¬Ø©\n",
    "    dataset_name = \"antique/test/non-offensive\"\n",
    "    terms_path = r\"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\vocabularies\\antique_train_terms.txt\"\n",
    "    output_path = r\"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\vocabularies\\Evaluation Query\\enhanced_queries_antique.joblib\"\n",
    "\n",
    "   generate_corrected_queries(\n",
    "        dataset_name=dataset_name,\n",
    "        terms_path=terms_path,\n",
    "        output_path=output_path\n",
    "    )\n"
   ],
   "id": "a9a0d42e5c167987",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ù…Ù†: antique/test/non-offensive\n",
      "ğŸ“– ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª Ù…Ù†: C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\vocabularies\\antique_train_terms.txt\n",
      "ğŸš€ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ”§ ØªØ­Ø³ÙŠÙ†: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 176/176 [30:44<00:00, 10.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ù…Ø­Ø³Ù†Ø© ÙÙŠ: C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\vocabularies\\Evaluation Query\\enhanced_queries_antique.joblib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T13:50:08.054444Z",
     "start_time": "2025-07-01T13:49:07.165318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from typing import List\n",
    "\n",
    "# âš™ï¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª BERT\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def embed_texts(texts: List[str], batch_size: int = 32) -> List[List[float]]:\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"ğŸ”¢ ØªÙ…Ø«ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            batch_embeds = outputs.last_hidden_state[:, 0, :]  # Ø§Ø³ØªØ®Ø¯Ø§Ù… [CLS]\n",
    "        embeddings.extend(batch_embeds.cpu().numpy())\n",
    "    return embeddings\n",
    "\n",
    "# âœ… Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©\n",
    "def represent_enhanced_queries(enhanced_queries_path, output_path):\n",
    "    print(\"ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ù…Ø­Ø³Ù†Ø©...\")\n",
    "    enhanced_data = joblib.load(enhanced_queries_path)\n",
    "    query_ids = list(enhanced_data.keys())\n",
    "    enhanced_texts = list(enhanced_data.values())\n",
    "\n",
    "    print(\"ğŸ”„ ØªÙ…Ø«ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… BERT...\")\n",
    "    embeddings = embed_texts(enhanced_texts, batch_size=32)\n",
    "\n",
    "    print(\"ğŸ’¾ Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬...\")\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    joblib.dump({\n",
    "        \"query_ids\": query_ids,\n",
    "        \"embeddings\": embeddings,\n",
    "        \"model_name\": MODEL_NAME\n",
    "    }, output_path)\n",
    "\n",
    "    print(f\"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙ…Ø«ÙŠÙ„Ø§Øª ÙÙŠ: {output_path}\")\n",
    "\n",
    "\n",
    "# === ØªÙ†ÙÙŠØ°\n",
    "if __name__ == \"__main__\":\n",
    "    enhanced_queries_path = r\"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\vocabularies\\Evaluation Query\\enhanced_queries_antique.joblib\"\n",
    "    output_path = r\"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\Query Processing\\Bertquery\\antique\\enhanced\\bert_enhanced_queries.joblib\"\n",
    "\n",
    "    represent_enhanced_queries(enhanced_queries_path, output_path)\n"
   ],
   "id": "b1949607e12fea7c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ù…Ø­Ø³Ù†Ø©...\n",
      "ğŸ”„ ØªÙ…Ø«ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… BERT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ”¢ ØªÙ…Ø«ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:03<00:00,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬...\n",
      "âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙ…Ø«ÙŠÙ„Ø§Øª ÙÙŠ: C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\Query Processing\\Bertquery\\antique\\enhanced\\bert_enhanced_queries.joblib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T14:04:50.983850Z",
     "start_time": "2025-07-01T14:04:41.351071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import re\n",
    "import joblib\n",
    "import nltk\n",
    "import ir_datasets\n",
    "from tqdm import tqdm\n",
    "from pymongo import MongoClient\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    return re.findall(r'\\b\\w+\\b', text)\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "def clean_text(text):\n",
    "    tokens = tokenize(text)\n",
    "    lemmas = lemmatize_tokens(tokens)\n",
    "    return \" \".join(lemmas)\n",
    "\n",
    "def build_hybrid_enhanced_representation(enhanced_queries_path, vectorizer_path, bert_path, output_path):\n",
    "    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "    print(\"ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...\")\n",
    "    enhanced_data = joblib.load(enhanced_queries_path)\n",
    "    vectorizer_data = joblib.load(vectorizer_path)\n",
    "    bert_data = joblib.load(bert_path)\n",
    "\n",
    "    vectorizer: TfidfVectorizer = vectorizer_data[\"vectorizer\"]\n",
    "    bert_embeddings = bert_data[\"embeddings\"]\n",
    "    bert_query_ids = bert_data[\"query_ids\"]\n",
    "    bert_model_name = bert_data[\"model_name\"]\n",
    "\n",
    "    bert_map = dict(zip(bert_query_ids, bert_embeddings))\n",
    "\n",
    "    query_docs = []\n",
    "    query_ids = []\n",
    "    original_texts = []\n",
    "    clean_texts = []\n",
    "    tfidf_indices_list = []\n",
    "    tfidf_values_list = []\n",
    "\n",
    "    print(\"ğŸ§¼ Ø¨Ù†Ø§Ø¡ ØªÙ…Ø«ÙŠÙ„Ø§Øª Ù‡Ø¬ÙŠÙ†Ø© Ù„Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ù…Ø­Ø³Ù†Ø©...\")\n",
    "    for qid, enhanced_text in tqdm(enhanced_data.items()):\n",
    "        if qid not in bert_map:\n",
    "            continue\n",
    "\n",
    "        cleaned = clean_text(enhanced_text)\n",
    "        tfidf_vector = vectorizer.transform([cleaned])\n",
    "        row = tfidf_vector.getrow(0).tocoo()\n",
    "\n",
    "        doc = {\n",
    "            \"query_id\": qid,\n",
    "            \"original_text\": enhanced_text,\n",
    "            \"clean_text\": cleaned,\n",
    "            \"bert_embedding\": bert_map[qid].tolist(),\n",
    "            \"tfidf_indices\": row.col.tolist(),\n",
    "            \"tfidf_values\": row.data.tolist()\n",
    "        }\n",
    "        query_docs.append(doc)\n",
    "        query_ids.append(qid)\n",
    "        original_texts.append(enhanced_text)\n",
    "        clean_texts.append(cleaned)\n",
    "        tfidf_indices_list.append(row.col.tolist())\n",
    "        tfidf_values_list.append(row.data.tolist())\n",
    "\n",
    "    # MongoDB (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)\n",
    "    client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "    db = client[\"ir_project\"]\n",
    "    collection = db[\"queries_enhanced_hybrid_antique\"]\n",
    "    collection.delete_many({})\n",
    "    collection.insert_many(query_docs)\n",
    "    print(f\"âœ… ØªÙ… ØªØ®Ø²ÙŠÙ† {len(query_docs)} Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù‡Ø¬ÙŠÙ† Ù…Ø­Ø³Ù‘Ù† ÙÙŠ: {collection.name}\")\n",
    "\n",
    "    # Joblib\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    joblib.dump({\n",
    "        \"query_ids\": query_ids,\n",
    "        \"original_texts\": original_texts,\n",
    "        \"clean_texts\": clean_texts,\n",
    "        \"bert_embeddings\": [bert_map[qid].tolist() for qid in query_ids],\n",
    "        \"tfidf_indices\": tfidf_indices_list,\n",
    "        \"tfidf_values\": tfidf_values_list,\n",
    "        \"bert_model_name\": bert_model_name\n",
    "    }, output_path)\n",
    "\n",
    "    print(f\"ğŸ“¦ ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙ…Ø«ÙŠÙ„Ø§Øª Ø§Ù„Ù‡Ø¬ÙŠÙ†Ø© Ø§Ù„Ù…Ø­Ø³Ù†Ø© ÙÙŠ: {output_path}\")\n",
    "\n",
    "\n",
    "# ğŸŸ© ØªÙ†ÙÙŠØ°\n",
    "if __name__ == \"__main__\":\n",
    "    enhanced_queries_path = r\"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\vocabularies\\Evaluation Query\\enhanced_queries_antique.joblib\"\n",
    "    vectorizer_path = r\"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\Data Representation\\TF-IDF\\antique\\train\\doc\\tfidf_data.joblib\"\n",
    "    bert_path = r\"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\Query Processing\\Bertquery\\antique\\enhanced\\bert_enhanced_queries.joblib\"\n",
    "    output_path = r\"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\Query Processing\\hybridQuery\\Antique\\enhanced\\hybrid_enhanced_queries.joblib\"\n",
    "\n",
    "    build_hybrid_enhanced_representation(\n",
    "        enhanced_queries_path=enhanced_queries_path,\n",
    "        vectorizer_path=vectorizer_path,\n",
    "        bert_path=bert_path,\n",
    "        output_path=output_path\n",
    "    )\n"
   ],
   "id": "d3348e271db11eae",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Azzam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Azzam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...\n",
      "ğŸ§¼ Ø¨Ù†Ø§Ø¡ ØªÙ…Ø«ÙŠÙ„Ø§Øª Ù‡Ø¬ÙŠÙ†Ø© Ù„Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ù…Ø­Ø³Ù†Ø©...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 176/176 [00:00<00:00, 578.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ØªÙ… ØªØ®Ø²ÙŠÙ† 176 Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù‡Ø¬ÙŠÙ† Ù…Ø­Ø³Ù‘Ù† ÙÙŠ: queries_enhanced_hybrid_antique\n",
      "ğŸ“¦ ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙ…Ø«ÙŠÙ„Ø§Øª Ø§Ù„Ù‡Ø¬ÙŠÙ†Ø© Ø§Ù„Ù…Ø­Ø³Ù†Ø© ÙÙŠ: C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\Query Processing\\hybridQuery\\Antique\\enhanced\\hybrid_enhanced_queries.joblib\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
