{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-03T09:39:00.122528Z",
     "start_time": "2025-07-03T09:39:00.108980Z"
    }
   },
   "source": [
    "import os\n",
    "import joblib\n",
    "import ir_datasets\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from difflib import get_close_matches\n",
    "\n",
    "# ==========================\n",
    "# 🧠 QueryRefiner - تصحيح فقط\n",
    "# ==========================\n",
    "class QueryRefinerCorrectOnly:\n",
    "    def __init__(self, processed_terms):\n",
    "        self.term_frequencies = Counter(processed_terms)\n",
    "        self.processed_terms = set(processed_terms)\n",
    "\n",
    "    def suggest_correction(self, query):\n",
    "        words = query.split()\n",
    "        corrected = []\n",
    "        for word in words:\n",
    "            matches = get_close_matches(word, self.processed_terms, n=1, cutoff=0.8)\n",
    "            if matches:\n",
    "                corrected.append(matches[0])\n",
    "            else:\n",
    "                corrected.append(word)\n",
    "        return corrected\n",
    "\n",
    "    def enhance(self, query):\n",
    "        corrected = self.suggest_correction(query)\n",
    "        return \" \".join(corrected)\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# 🔄 تحسين الاستعلامات\n",
    "# ==========================\n",
    "def generate_corrected_queries(dataset_name, terms_path, output_path, batch_size=1000):\n",
    "    print(f\"📥 تحميل الاستعلامات من: {dataset_name}\")\n",
    "    dataset = ir_datasets.load(dataset_name)\n",
    "    queries = list(dataset.queries_iter())\n",
    "\n",
    "    print(f\"📖 تحميل المفردات من: {terms_path}\")\n",
    "    with open(terms_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        terms = [line.strip().lower() for line in f if line.strip()]\n",
    "    refiner = QueryRefinerCorrectOnly(terms)\n",
    "\n",
    "    enhanced_queries = {}\n",
    "\n",
    "    print(\"🔧 تصحيح الاستعلامات فقط...\")\n",
    "    for i in range(0, len(queries), batch_size):\n",
    "        batch = queries[i:i+batch_size]\n",
    "        for q in tqdm(batch, desc=f\"🔤 دفعة {i//batch_size+1}\"):\n",
    "            enhanced = refiner.enhance(q.text)\n",
    "            enhanced_queries[q.query_id] = enhanced\n",
    "\n",
    "        # حفظ مؤقت لكل دفعة\n",
    "        partial_path = output_path.replace(\".joblib\", f\"_corrected_part{i//batch_size+1}.joblib\")\n",
    "        os.makedirs(os.path.dirname(partial_path), exist_ok=True)\n",
    "        joblib.dump(enhanced_queries, partial_path)\n",
    "        print(f\"💾 تم حفظ الدفعة {i//batch_size+1} في: {partial_path}\")\n",
    "\n",
    "    joblib.dump(enhanced_queries, output_path)\n",
    "    print(f\"✅ تم حفظ الاستعلامات المصححة في: {output_path}\")\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T09:41:39.119968Z",
     "start_time": "2025-07-03T09:39:15.522421Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==========================\n",
    "# ▶️ تنفيذ السكربت\n",
    "# ==========================\n",
    "if __name__ == \"__main__\":\n",
    "    # 🔧 عدّل هذه القيم حسب الحاجة\n",
    "    dataset_name = \"beir/quora/test\"\n",
    "    terms_path = r\"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\vocabularies\\beir_quora_test_terms.txt\"\n",
    "    output_path = r\"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\vocabularies\\Evaluation Query\\enhanced_queries.joblib\"\n",
    "    batch_size = 100\n",
    "\n",
    "    generate_corrected_queries(\n",
    "        dataset_name=dataset_name,\n",
    "        terms_path=terms_path,\n",
    "        output_path=output_path,\n",
    "        batch_size=batch_size\n",
    "    )\n"
   ],
   "id": "cb7aba01e728cf7d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 تحميل الاستعلامات من: beir/quora/test\n",
      "📖 تحميل المفردات من: C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\vocabularies\\beir_quora_test_terms.txt\n",
      "🔧 تصحيح الاستعلامات فقط...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔤 دفعة 1: 100%|██████████| 100/100 [01:51<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 تم حفظ الدفعة 1 في: C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\vocabularies\\Evaluation Query\\enhanced_queries_corrected_part1.joblib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔤 دفعة 2:  30%|███       | 30/100 [00:31<01:12,  1.04s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 11\u001B[39m\n\u001B[32m      8\u001B[39m output_path = \u001B[33mr\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mC:\u001B[39m\u001B[33m\\\u001B[39m\u001B[33mUsers\u001B[39m\u001B[33m\\\u001B[39m\u001B[33mAzzam\u001B[39m\u001B[33m\\\u001B[39m\u001B[33mPycharmProjects\u001B[39m\u001B[33m\\\u001B[39m\u001B[33mPythonProject\u001B[39m\u001B[33m\\\u001B[39m\u001B[33mvocabularies\u001B[39m\u001B[33m\\\u001B[39m\u001B[33mEvaluation Query\u001B[39m\u001B[33m\\\u001B[39m\u001B[33menhanced_queries.joblib\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m      9\u001B[39m batch_size = \u001B[32m100\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m11\u001B[39m \u001B[43mgenerate_corrected_queries\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     12\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdataset_name\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdataset_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     13\u001B[39m \u001B[43m    \u001B[49m\u001B[43mterms_path\u001B[49m\u001B[43m=\u001B[49m\u001B[43mterms_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     14\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_path\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     15\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbatch_size\u001B[49m\n\u001B[32m     16\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 51\u001B[39m, in \u001B[36mgenerate_corrected_queries\u001B[39m\u001B[34m(dataset_name, terms_path, output_path, batch_size)\u001B[39m\n\u001B[32m     49\u001B[39m batch = queries[i:i+batch_size]\n\u001B[32m     50\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m q \u001B[38;5;129;01min\u001B[39;00m tqdm(batch, desc=\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m🔤 دفعة \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi//batch_size+\u001B[32m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m):\n\u001B[32m---> \u001B[39m\u001B[32m51\u001B[39m     enhanced = \u001B[43mrefiner\u001B[49m\u001B[43m.\u001B[49m\u001B[43menhance\u001B[49m\u001B[43m(\u001B[49m\u001B[43mq\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     52\u001B[39m     enhanced_queries[q.query_id] = enhanced\n\u001B[32m     54\u001B[39m \u001B[38;5;66;03m# حفظ مؤقت لكل دفعة\u001B[39;00m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 28\u001B[39m, in \u001B[36mQueryRefinerCorrectOnly.enhance\u001B[39m\u001B[34m(self, query)\u001B[39m\n\u001B[32m     27\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34menhance\u001B[39m(\u001B[38;5;28mself\u001B[39m, query):\n\u001B[32m---> \u001B[39m\u001B[32m28\u001B[39m     corrected = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msuggest_correction\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     29\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33m \u001B[39m\u001B[33m\"\u001B[39m.join(corrected)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 20\u001B[39m, in \u001B[36mQueryRefinerCorrectOnly.suggest_correction\u001B[39m\u001B[34m(self, query)\u001B[39m\n\u001B[32m     18\u001B[39m corrected = []\n\u001B[32m     19\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m words:\n\u001B[32m---> \u001B[39m\u001B[32m20\u001B[39m     matches = \u001B[43mget_close_matches\u001B[49m\u001B[43m(\u001B[49m\u001B[43mword\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mprocessed_terms\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcutoff\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.8\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     21\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m matches:\n\u001B[32m     22\u001B[39m         corrected.append(matches[\u001B[32m0\u001B[39m])\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\difflib.py:705\u001B[39m, in \u001B[36mget_close_matches\u001B[39m\u001B[34m(word, possibilities, n, cutoff)\u001B[39m\n\u001B[32m    702\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m possibilities:\n\u001B[32m    703\u001B[39m     s.set_seq1(x)\n\u001B[32m    704\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m s.real_quick_ratio() >= cutoff \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[32m--> \u001B[39m\u001B[32m705\u001B[39m        \u001B[43ms\u001B[49m\u001B[43m.\u001B[49m\u001B[43mquick_ratio\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m >= cutoff \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[32m    706\u001B[39m        s.ratio() >= cutoff:\n\u001B[32m    707\u001B[39m         result.append((s.ratio(), x))\n\u001B[32m    709\u001B[39m \u001B[38;5;66;03m# Move the best scorers to head of list\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\difflib.py:-1\u001B[39m, in \u001B[36mSequenceMatcher.quick_ratio\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m      0\u001B[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T12:39:12.004839Z",
     "start_time": "2025-07-01T12:08:27.350281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==========================\n",
    "# ▶️ تنفيذ السكربت\n",
    "# ==========================\n",
    "if __name__ == \"__main__\":\n",
    "    # 🔧 عدّل هذه القيم حسب الحاجة\n",
    "    dataset_name = \"antique/test/non-offensive\"\n",
    "    terms_path = r\"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\vocabularies\\antique_train_terms.txt\"\n",
    "    output_path = r\"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\vocabularies\\Evaluation Query\\enhanced_queries_antique.joblib\"\n",
    "\n",
    "   generate_corrected_queries(\n",
    "        dataset_name=dataset_name,\n",
    "        terms_path=terms_path,\n",
    "        output_path=output_path\n",
    "    )\n"
   ],
   "id": "a9a0d42e5c167987",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 تحميل الاستعلامات من: antique/test/non-offensive\n",
      "📖 تحميل المفردات من: C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\vocabularies\\antique_train_terms.txt\n",
      "🚀 تحسين الاستعلامات...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔧 تحسين: 100%|██████████| 176/176 [30:44<00:00, 10.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ تم حفظ الاستعلامات المحسنة في: C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\vocabularies\\Evaluation Query\\enhanced_queries_antique.joblib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T13:50:08.054444Z",
     "start_time": "2025-07-01T13:49:07.165318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from typing import List\n",
    "\n",
    "# ⚙️ إعدادات BERT\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def embed_texts(texts: List[str], batch_size: int = 32) -> List[List[float]]:\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"🔢 تمثيل الاستعلامات\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            batch_embeds = outputs.last_hidden_state[:, 0, :]  # استخدام [CLS]\n",
    "        embeddings.extend(batch_embeds.cpu().numpy())\n",
    "    return embeddings\n",
    "\n",
    "# ✅ الدالة الرئيسية\n",
    "def represent_enhanced_queries(enhanced_queries_path, output_path):\n",
    "    print(\"📥 تحميل الاستعلامات المحسنة...\")\n",
    "    enhanced_data = joblib.load(enhanced_queries_path)\n",
    "    query_ids = list(enhanced_data.keys())\n",
    "    enhanced_texts = list(enhanced_data.values())\n",
    "\n",
    "    print(\"🔄 تمثيل الاستعلامات باستخدام BERT...\")\n",
    "    embeddings = embed_texts(enhanced_texts, batch_size=32)\n",
    "\n",
    "    print(\"💾 حفظ النتائج...\")\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    joblib.dump({\n",
    "        \"query_ids\": query_ids,\n",
    "        \"embeddings\": embeddings,\n",
    "        \"model_name\": MODEL_NAME\n",
    "    }, output_path)\n",
    "\n",
    "    print(f\"✅ تم حفظ التمثيلات في: {output_path}\")\n",
    "\n",
    "\n",
    "# === تنفيذ\n",
    "if __name__ == \"__main__\":\n",
    "    enhanced_queries_path = r\"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\vocabularies\\Evaluation Query\\enhanced_queries_antique.joblib\"\n",
    "    output_path = r\"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\Query Processing\\Bertquery\\antique\\enhanced\\bert_enhanced_queries.joblib\"\n",
    "\n",
    "    represent_enhanced_queries(enhanced_queries_path, output_path)\n"
   ],
   "id": "b1949607e12fea7c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 تحميل الاستعلامات المحسنة...\n",
      "🔄 تمثيل الاستعلامات باستخدام BERT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔢 تمثيل الاستعلامات: 100%|██████████| 6/6 [00:03<00:00,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 حفظ النتائج...\n",
      "✅ تم حفظ التمثيلات في: C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\Query Processing\\Bertquery\\antique\\enhanced\\bert_enhanced_queries.joblib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T14:04:50.983850Z",
     "start_time": "2025-07-01T14:04:41.351071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import re\n",
    "import joblib\n",
    "import nltk\n",
    "import ir_datasets\n",
    "from tqdm import tqdm\n",
    "from pymongo import MongoClient\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    return re.findall(r'\\b\\w+\\b', text)\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "def clean_text(text):\n",
    "    tokens = tokenize(text)\n",
    "    lemmas = lemmatize_tokens(tokens)\n",
    "    return \" \".join(lemmas)\n",
    "\n",
    "def build_hybrid_enhanced_representation(enhanced_queries_path, vectorizer_path, bert_path, output_path):\n",
    "    # تحميل البيانات\n",
    "    print(\"📥 تحميل البيانات...\")\n",
    "    enhanced_data = joblib.load(enhanced_queries_path)\n",
    "    vectorizer_data = joblib.load(vectorizer_path)\n",
    "    bert_data = joblib.load(bert_path)\n",
    "\n",
    "    vectorizer: TfidfVectorizer = vectorizer_data[\"vectorizer\"]\n",
    "    bert_embeddings = bert_data[\"embeddings\"]\n",
    "    bert_query_ids = bert_data[\"query_ids\"]\n",
    "    bert_model_name = bert_data[\"model_name\"]\n",
    "\n",
    "    bert_map = dict(zip(bert_query_ids, bert_embeddings))\n",
    "\n",
    "    query_docs = []\n",
    "    query_ids = []\n",
    "    original_texts = []\n",
    "    clean_texts = []\n",
    "    tfidf_indices_list = []\n",
    "    tfidf_values_list = []\n",
    "\n",
    "    print(\"🧼 بناء تمثيلات هجينة للاستعلامات المحسنة...\")\n",
    "    for qid, enhanced_text in tqdm(enhanced_data.items()):\n",
    "        if qid not in bert_map:\n",
    "            continue\n",
    "\n",
    "        cleaned = clean_text(enhanced_text)\n",
    "        tfidf_vector = vectorizer.transform([cleaned])\n",
    "        row = tfidf_vector.getrow(0).tocoo()\n",
    "\n",
    "        doc = {\n",
    "            \"query_id\": qid,\n",
    "            \"original_text\": enhanced_text,\n",
    "            \"clean_text\": cleaned,\n",
    "            \"bert_embedding\": bert_map[qid].tolist(),\n",
    "            \"tfidf_indices\": row.col.tolist(),\n",
    "            \"tfidf_values\": row.data.tolist()\n",
    "        }\n",
    "        query_docs.append(doc)\n",
    "        query_ids.append(qid)\n",
    "        original_texts.append(enhanced_text)\n",
    "        clean_texts.append(cleaned)\n",
    "        tfidf_indices_list.append(row.col.tolist())\n",
    "        tfidf_values_list.append(row.data.tolist())\n",
    "\n",
    "    # MongoDB (اختياري)\n",
    "    client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "    db = client[\"ir_project\"]\n",
    "    collection = db[\"queries_enhanced_hybrid_antique\"]\n",
    "    collection.delete_many({})\n",
    "    collection.insert_many(query_docs)\n",
    "    print(f\"✅ تم تخزين {len(query_docs)} استعلام هجين محسّن في: {collection.name}\")\n",
    "\n",
    "    # Joblib\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    joblib.dump({\n",
    "        \"query_ids\": query_ids,\n",
    "        \"original_texts\": original_texts,\n",
    "        \"clean_texts\": clean_texts,\n",
    "        \"bert_embeddings\": [bert_map[qid].tolist() for qid in query_ids],\n",
    "        \"tfidf_indices\": tfidf_indices_list,\n",
    "        \"tfidf_values\": tfidf_values_list,\n",
    "        \"bert_model_name\": bert_model_name\n",
    "    }, output_path)\n",
    "\n",
    "    print(f\"📦 تم حفظ التمثيلات الهجينة المحسنة في: {output_path}\")\n",
    "\n",
    "\n",
    "# 🟩 تنفيذ\n",
    "if __name__ == \"__main__\":\n",
    "    enhanced_queries_path = r\"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\vocabularies\\Evaluation Query\\enhanced_queries_antique.joblib\"\n",
    "    vectorizer_path = r\"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\Data Representation\\TF-IDF\\antique\\train\\doc\\tfidf_data.joblib\"\n",
    "    bert_path = r\"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\Query Processing\\Bertquery\\antique\\enhanced\\bert_enhanced_queries.joblib\"\n",
    "    output_path = r\"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\Query Processing\\hybridQuery\\Antique\\enhanced\\hybrid_enhanced_queries.joblib\"\n",
    "\n",
    "    build_hybrid_enhanced_representation(\n",
    "        enhanced_queries_path=enhanced_queries_path,\n",
    "        vectorizer_path=vectorizer_path,\n",
    "        bert_path=bert_path,\n",
    "        output_path=output_path\n",
    "    )\n"
   ],
   "id": "d3348e271db11eae",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Azzam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Azzam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 تحميل البيانات...\n",
      "🧼 بناء تمثيلات هجينة للاستعلامات المحسنة...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 176/176 [00:00<00:00, 578.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ تم تخزين 176 استعلام هجين محسّن في: queries_enhanced_hybrid_antique\n",
      "📦 تم حفظ التمثيلات الهجينة المحسنة في: C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\Query Processing\\hybridQuery\\Antique\\enhanced\\hybrid_enhanced_queries.joblib\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
