{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-03T14:39:21.612797Z",
     "start_time": "2025-07-03T14:25:11.242615Z"
    }
   },
   "source": [
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from bertopic import BERTopic\n",
    "\n",
    "# ============================\n",
    "# ğŸ› ï¸ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª\n",
    "# ============================\n",
    "\n",
    "datasets = {\n",
    "    \"antique_train\": r\"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\Data Pre-Processing\\antique\\train\\doc\\docs.json\",\n",
    "    \"beir_quora_test\": r\"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\Data Pre-Processing\\beir\\quora\\test\\doc\\docs.json\"\n",
    "}\n",
    "\n",
    "embeddings_base_dir = r\"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\Data Representation\\Bert\"\n",
    "output_base_dir = r\"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\TopicResults\"\n",
    "os.makedirs(output_base_dir, exist_ok=True)\n",
    "\n",
    "for dataset_name, json_path in datasets.items():\n",
    "    if not os.path.exists(json_path):\n",
    "        print(f\"âŒ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ù„Ù docs.json ÙÙŠ {dataset_name} - Ø³ÙŠØªÙ… ØªØ®Ø·ÙŠÙ‡.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nğŸ“‚ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¯Ø§ØªØ§: {dataset_name}\")\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        docs_list = json.load(f)\n",
    "\n",
    "    docs, doc_ids = [], []\n",
    "    for doc in tqdm(docs_list, desc=\"ğŸ“„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚\"):\n",
    "        if \"clean_text\" in doc and doc[\"clean_text\"].strip():\n",
    "            docs.append(doc[\"clean_text\"])\n",
    "            doc_ids.append(doc[\"doc_id\"])\n",
    "\n",
    "    print(f\"âœ… Loaded {len(docs)} documents.\")\n",
    "\n",
    "    embedding_path = os.path.join(\n",
    "        embeddings_base_dir,\n",
    "        dataset_name.replace(\"_\", os.sep),\n",
    "        \"doc\",\n",
    "        \"bert_embedding.joblib\"\n",
    "    )\n",
    "\n",
    "    if not os.path.exists(embedding_path):\n",
    "        print(f\"âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù„Ù Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª ÙÙŠ {embedding_path}\")\n",
    "        continue\n",
    "\n",
    "    embedding_data = joblib.load(embedding_path)\n",
    "    if embedding_data[\"doc_ids\"] != doc_ids:\n",
    "        print(f\"âŒ ØªØ±ØªÙŠØ¨ doc_ids Ù„Ø§ ÙŠØªØ·Ø§Ø¨Ù‚ ÙÙŠ {dataset_name}\")\n",
    "        continue\n",
    "\n",
    "    embeddings = np.array(embedding_data[\"embeddings_matrix\"])\n",
    "\n",
    "    print(\"ğŸ§  Fitting BERTopic model...\")\n",
    "    topic_model = BERTopic(language=\"english\", verbose=True)\n",
    "    topics, probs = topic_model.fit_transform(docs, embeddings)\n",
    "\n",
    "    # ============================\n",
    "    # ğŸ” Ø¥Ù†Ø´Ø§Ø¡ ØªÙ…Ø«ÙŠÙ„ Ù„ÙƒÙ„ ØªÙˆØ¨ÙŠÙƒ\n",
    "    # ============================\n",
    "\n",
    "    topic_embeddings = {}\n",
    "    for topic_id in set(topics):\n",
    "        indices = [i for i, t in enumerate(topics) if t == topic_id]\n",
    "        topic_embeds = embeddings[indices]\n",
    "        topic_embeddings[topic_id] = topic_embeds.mean(axis=0)\n",
    "\n",
    "    # ğŸ§ª Ø­ÙØ¸ ÙƒÙ„ Ø´ÙŠØ¡\n",
    "    output_path = os.path.join(output_base_dir, f\"{dataset_name}_bertopic_results.joblib\")\n",
    "    joblib.dump({\n",
    "        \"topics\": topics,\n",
    "        \"probs\": probs,\n",
    "        \"doc_ids\": doc_ids,\n",
    "        \"model\": topic_model,\n",
    "        \"topic_embeddings\": topic_embeddings  # â¬…ï¸ Ø§Ù„ØªÙ…Ø«ÙŠÙ„Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©\n",
    "    }, output_path)\n",
    "\n",
    "    print(f\"ğŸ’¾ Results saved to: {output_path}\")\n",
    "\n",
    "    try:\n",
    "        print(\"ğŸ“Š Visualizing topics...\")\n",
    "        fig = topic_model.visualize_topics()\n",
    "        html_path = os.path.join(output_base_dir, f\"{dataset_name}_topics_visualization.html\")\n",
    "        fig.write_html(html_path)\n",
    "        print(f\"ğŸŒ Visualization saved as HTML: {html_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Visualization failed: {e}\")\n",
    "\n",
    "    print(\"\\nğŸ” First 10 document-topic pairs:\")\n",
    "    for i in range(min(10, len(doc_ids))):\n",
    "        print(f\"Doc ID: {doc_ids[i]}  --> Topic: {topics[i]}\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.6 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\Azzam\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\Azzam\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\Azzam\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3116, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3171, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3394, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3639, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Azzam\\AppData\\Local\\Temp\\ipykernel_10648\\3145694699.py\", line 6, in <module>\n",
      "    from bertopic import BERTopic\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\bertopic\\__init__.py\", line 3, in <module>\n",
      "    from bertopic._bertopic import BERTopic\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\bertopic\\_bertopic.py\", line 58, in <module>\n",
      "    from bertopic.backend import BaseEmbedder\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\bertopic\\backend\\__init__.py\", line 22, in <module>\n",
      "    from bertopic.backend._multimodal import MultiModalBackend\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\bertopic\\backend\\_multimodal.py\", line 5, in <module>\n",
      "    from sentence_transformers import SentenceTransformer\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\sentence_transformers\\__init__.py\", line 10, in <module>\n",
      "    from sentence_transformers.backend import (\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\sentence_transformers\\backend.py\", line 11, in <module>\n",
      "    from sentence_transformers.util import disable_datasets_caching, is_datasets_available\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\sentence_transformers\\util.py\", line 20, in <module>\n",
      "    import torch\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\torch\\__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\torch\\functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“‚ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¯Ø§ØªØ§: antique_train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 403666/403666 [00:00<00:00, 1446849.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 401768 documents.\n",
      "ğŸ§  Fitting BERTopic model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 17:25:55,563 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-07-03 17:33:46,655 - BERTopic - Dimensionality - Completed âœ“\n",
      "2025-07-03 17:33:46,682 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-07-03 17:35:08,923 - BERTopic - Cluster - Completed âœ“\n",
      "2025-07-03 17:35:09,062 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-07-03 17:35:27,477 - BERTopic - Representation - Completed âœ“\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Results saved to: C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\TopicResults\\antique_train_bertopic_results.joblib\n",
      "ğŸ“Š Visualizing topics...\n",
      "ğŸŒ Visualization saved as HTML: C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\TopicResults\\antique_train_topics_visualization.html\n",
      "\n",
      "ğŸ” First 10 document-topic pairs:\n",
      "Doc ID: 2020338_0  --> Topic: 1381\n",
      "Doc ID: 2020338_1  --> Topic: 191\n",
      "Doc ID: 2020338_2  --> Topic: -1\n",
      "Doc ID: 2020338_3  --> Topic: -1\n",
      "Doc ID: 2874684_0  --> Topic: 1171\n",
      "Doc ID: 2874684_1  --> Topic: 1317\n",
      "Doc ID: 4193114_0  --> Topic: -1\n",
      "Doc ID: 4193114_1  --> Topic: -1\n",
      "Doc ID: 1908421_0  --> Topic: 1572\n",
      "Doc ID: 1908421_1  --> Topic: 1572\n",
      "\n",
      "ğŸ“‚ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¯Ø§ØªØ§: beir_quora_test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 522931/522931 [00:00<00:00, 1778696.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 522719 documents.\n",
      "âŒ ØªØ±ØªÙŠØ¨ doc_ids ÙÙŠ Ù…Ù„Ù Ø§Ù„ØªØ¶Ù…ÙŠÙ† Ù„Ø§ ÙŠØ·Ø§Ø¨Ù‚ Ù…Ù„Ù Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ ÙÙŠ beir_quora_test\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T18:52:10.631360Z",
     "start_time": "2025-07-06T18:12:08.409339Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from bertopic import BERTopic\n",
    "\n",
    "# ============================\n",
    "# ğŸ› ï¸ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª\n",
    "# ============================\n",
    "\n",
    "json_path = r\"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\Data Pre-Processing\\beir\\quora\\test\\doc\\docs.json\"\n",
    "embedding_path = r\"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\Data Representation\\Bert\\beir\\quora\\test\\doc\\bert_embedding.joblib\"\n",
    "output_dir = r\"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\TopicResults\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ============================\n",
    "# ğŸ“„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚\n",
    "# ============================\n",
    "\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    docs_list = json.load(f)\n",
    "\n",
    "docs = []\n",
    "doc_ids = []\n",
    "for doc in tqdm(docs_list, desc=\"ğŸ“„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚\"):\n",
    "    if \"clean_text\" in doc and doc[\"clean_text\"].strip():\n",
    "        docs.append(doc[\"clean_text\"])\n",
    "        doc_ids.append(doc[\"doc_id\"])\n",
    "\n",
    "print(f\"âœ… Loaded {len(docs)} documents.\")\n",
    "\n",
    "# ============================\n",
    "# ğŸ”Œ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª\n",
    "# ============================\n",
    "\n",
    "embedding_data = joblib.load(embedding_path)\n",
    "embedding_doc_ids = embedding_data[\"doc_ids\"]\n",
    "embedding_matrix = embedding_data[\"embeddings_matrix\"]\n",
    "\n",
    "print(\"ğŸ” Ø¥Ø¹Ø§Ø¯Ø© ØªØ±ØªÙŠØ¨ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª Ø­Ø³Ø¨ ØªØ±ØªÙŠØ¨ docs.json...\")\n",
    "\n",
    "# ğŸ§  Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ù…ÙˆØ³ ID â†’ embedding\n",
    "id_to_embedding = {doc_id: emb for doc_id, emb in zip(embedding_doc_ids, embedding_matrix)}\n",
    "\n",
    "# ğŸ§© Ø¥Ø¹Ø§Ø¯Ø© ØªØ±ØªÙŠØ¨ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª\n",
    "reordered_embeddings = []\n",
    "new_docs = []\n",
    "new_doc_ids = []\n",
    "for i, doc_id in enumerate(doc_ids):\n",
    "    if doc_id in id_to_embedding:\n",
    "        reordered_embeddings.append(id_to_embedding[doc_id])\n",
    "        new_docs.append(docs[i])\n",
    "        new_doc_ids.append(doc_id)\n",
    "\n",
    "docs = new_docs\n",
    "doc_ids = new_doc_ids\n",
    "reordered_embeddings = np.array(reordered_embeddings)\n",
    "\n",
    "print(f\"ğŸ”¢ Ø¹Ø¯Ø¯ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª Ø¨Ø¹Ø¯ Ø§Ù„ØªØ±ØªÙŠØ¨: {reordered_embeddings.shape[0]}\")\n",
    "\n",
    "# ============================\n",
    "# ğŸ§  BERTopic\n",
    "# ============================\n",
    "\n",
    "print(\"ğŸ§  Fitting BERTopic model...\")\n",
    "topic_model = BERTopic(language=\"english\", verbose=True)\n",
    "topics, probs = topic_model.fit_transform(docs, reordered_embeddings)\n",
    "\n",
    "# ğŸ”§ ØªÙ‚Ù„ÙŠØµ Ø¹Ø¯Ø¯ Ø§Ù„ØªÙˆØ¨ÙŠÙƒØ§Øª\n",
    "print(\"ğŸ”§ Reducing topics to 4500...\")\n",
    "topic_model.reduce_topics(docs, nr_topics=4500)\n",
    "\n",
    "# ============================\n",
    "# ğŸ¯ ØªÙˆÙ„ÙŠØ¯ ØªÙ…Ø«ÙŠÙ„ Ù„ÙƒÙ„ ØªÙˆØ¨ÙŠÙƒ\n",
    "# ============================\n",
    "\n",
    "print(\"ğŸ§  Generating topic embeddings...\")\n",
    "topic_embeddings = {}\n",
    "for topic_id in set(topics):\n",
    "    indices = [i for i, t in enumerate(topics) if t == topic_id]\n",
    "    topic_vector = reordered_embeddings[indices].mean(axis=0)\n",
    "    topic_embeddings[topic_id] = topic_vector\n",
    "\n",
    "# ============================\n",
    "# ğŸ’¾ Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n",
    "# ============================\n",
    "\n",
    "output_path = os.path.join(output_dir, \"beir_quora_test_bertopic_results.joblib\")\n",
    "joblib.dump({\n",
    "    \"topics\": topics,\n",
    "    \"probs\": probs,\n",
    "    \"doc_ids\": doc_ids,\n",
    "    \"model\": topic_model,\n",
    "    \"topic_embeddings\": topic_embeddings\n",
    "}, output_path)\n",
    "\n",
    "print(f\"ğŸ’¾ Results saved to: {output_path}\")\n",
    "\n",
    "# ============================\n",
    "# ğŸŒ Ø­ÙØ¸ Ø§Ù„ØªØµÙˆÙ‘Ø±\n",
    "# ============================\n",
    "\n",
    "try:\n",
    "    print(\"ğŸ“Š Visualizing topics...\")\n",
    "    fig = topic_model.visualize_topics()\n",
    "    html_path = os.path.join(output_dir, \"beir_quora_test_topics_visualization.html\")\n",
    "    fig.write_html(html_path)\n",
    "    print(f\"ğŸŒ Visualization saved as HTML.\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ ÙØ´Ù„ ÙÙŠ Ø­ÙØ¸ Ø§Ù„ØªØµÙˆÙ‘Ø±: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nğŸ” First 10 document-topic pairs:\")\n",
    "for i in range(min(10, len(doc_ids))):\n",
    "    print(f\"Doc ID: {doc_ids[i]}  --> Topic: {topics[i]}\")"
   ],
   "id": "bf0f764042eb60c6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "ğŸ“„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 522931/522931 [00:00<00:00, 794727.97it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 522719 documents.\n",
      "ğŸ” Ø¥Ø¹Ø§Ø¯Ø© ØªØ±ØªÙŠØ¨ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª Ø­Ø³Ø¨ ØªØ±ØªÙŠØ¨ docs.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 21:13:31,121 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¢ Ø¹Ø¯Ø¯ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª Ø¨Ø¹Ø¯ Ø§Ù„ØªØ±ØªÙŠØ¨: 522719\n",
      "ğŸ§  Fitting BERTopic model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 21:30:27,779 - BERTopic - Dimensionality - Completed âœ“\n",
      "2025-07-06 21:30:27,864 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-07-06 21:35:06,442 - BERTopic - Cluster - Completed âœ“\n",
      "2025-07-06 21:35:06,802 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-07-06 21:35:43,910 - BERTopic - Representation - Completed âœ“\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Reducing topics to 4500...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 21:36:59,162 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2025-07-06 21:37:07,121 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-07-06 21:37:48,333 - BERTopic - Representation - Completed âœ“\n",
      "2025-07-06 21:37:48,567 - BERTopic - Topic reduction - Reduced number of topics from 8926 to 4500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  Generating topic embeddings...\n",
      "ğŸ’¾ Results saved to: C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\TopicResults\\beir_quora_test_bertopic_results.joblib\n",
      "ğŸ“Š Visualizing topics...\n",
      "ğŸŒ Visualization saved as HTML.\n",
      "\n",
      "ğŸ” First 10 document-topic pairs:\n",
      "Doc ID: 1  --> Topic: 384\n",
      "Doc ID: 2  --> Topic: 2454\n",
      "Doc ID: 3  --> Topic: -1\n",
      "Doc ID: 4  --> Topic: -1\n",
      "Doc ID: 5  --> Topic: 462\n",
      "Doc ID: 6  --> Topic: -1\n",
      "Doc ID: 7  --> Topic: 1002\n",
      "Doc ID: 8  --> Topic: -1\n",
      "Doc ID: 9  --> Topic: -1\n",
      "Doc ID: 10  --> Topic: -1\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T10:22:16.825386Z",
     "start_time": "2025-07-07T10:21:18.006406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# ============================\n",
    "# ğŸ› ï¸ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª\n",
    "# ============================\n",
    "input_path = r\"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\TopicResults\\beir_quora_test_bertopic_results.joblib\"\n",
    "output_path = r\"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\TopicResults\\merged_topics_1500.joblib\"\n",
    "TARGET_NUM_TOPICS = 5\n",
    "\n",
    "# ============================\n",
    "# ğŸ“¦ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªÙˆØ¨ÙŠÙƒØ§Øª\n",
    "# ============================\n",
    "data = joblib.load(input_path)\n",
    "\n",
    "original_topics = data[\"topics\"]  # Ù‚Ø§Ø¦Ù…Ø©: Ù…ÙˆØ¶ÙˆØ¹ Ù„ÙƒÙ„ ÙˆØ«ÙŠÙ‚Ø©\n",
    "doc_ids = data[\"doc_ids\"]\n",
    "probs = data[\"probs\"]\n",
    "topic_embeddings = data[\"topic_embeddings\"]  # dict\n",
    "model = data.get(\"model\", None)\n",
    "\n",
    "topic_ids = list(topic_embeddings.keys())\n",
    "topic_vectors = np.vstack([topic_embeddings[tid] for tid in topic_ids])\n",
    "\n",
    "# ============================\n",
    "# ğŸ¤ Ø¯Ù…Ø¬ Ø§Ù„ØªÙˆØ¨ÙŠÙƒØ§Øª Ø¹Ø¨Ø± KMeans\n",
    "# ============================\n",
    "print(f\"ğŸ“‰ Clustering {len(topic_ids)} topics â†’ {TARGET_NUM_TOPICS} clusters...\")\n",
    "kmeans = KMeans(n_clusters=TARGET_NUM_TOPICS, random_state=42)\n",
    "new_topic_assignments = kmeans.fit_predict(topic_vectors)\n",
    "\n",
    "# mapping: old_topic_id â†’ new_topic_id\n",
    "old_to_new_topic = {old: int(new) for old, new in zip(topic_ids, new_topic_assignments)}\n",
    "\n",
    "# ============================\n",
    "# ğŸ” ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„ØªÙˆØ¨ÙŠÙƒ Ù„ÙƒÙ„ ÙˆØ«ÙŠÙ‚Ø©\n",
    "# ============================\n",
    "new_topics = [old_to_new_topic.get(tid, -1) for tid in original_topics]\n",
    "\n",
    "# ============================\n",
    "# ğŸ§  Ø­Ø³Ø§Ø¨ Ø§Ù„ØªÙ…Ø«ÙŠÙ„ Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ù„ÙƒÙ„ ØªÙˆØ¨ÙŠÙƒ\n",
    "# ============================\n",
    "print(\"ğŸ“ Recomputing merged topic embeddings...\")\n",
    "new_topic_embeddings = {}\n",
    "for new_topic_id in range(TARGET_NUM_TOPICS):\n",
    "    indices = [i for i, t in enumerate(new_topics) if t == new_topic_id]\n",
    "    if indices:\n",
    "        emb_matrix = np.array([topic_vectors[topic_ids.index(original_topics[i])] for i in indices])\n",
    "        new_topic_embeddings[new_topic_id] = emb_matrix.mean(axis=0)\n",
    "\n",
    "# ============================\n",
    "# ğŸ’¾ Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n",
    "# ============================\n",
    "joblib.dump({\n",
    "    \"topics\": new_topics,\n",
    "    \"probs\": probs,\n",
    "    \"doc_ids\": doc_ids,\n",
    "    \"model\": model,\n",
    "    \"topic_embeddings\": new_topic_embeddings\n",
    "}, output_path)\n",
    "\n",
    "print(f\"âœ… Merged topics saved to: {output_path}\")\n"
   ],
   "id": "a21b2e17675bfbd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‰ Clustering 8926 topics â†’ 5 clusters...\n",
      "ğŸ“ Recomputing merged topic embeddings...\n",
      "âœ… Merged topics saved to: C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\TopicResults\\merged_topics_1500.joblib\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T14:13:11.601510Z",
     "start_time": "2025-07-06T14:12:35.322813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import bertopic\n",
    "print(bertopic.__version__)\n"
   ],
   "id": "c6560705203e102b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17.0\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
