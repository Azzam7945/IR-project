{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-03T14:39:21.612797Z",
     "start_time": "2025-07-03T14:25:11.242615Z"
    }
   },
   "source": [
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from bertopic import BERTopic\n",
    "\n",
    "# ============================\n",
    "# 🛠️ الإعدادات\n",
    "# ============================\n",
    "\n",
    "datasets = {\n",
    "    \"antique_train\": r\"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\Data Pre-Processing\\antique\\train\\doc\\docs.json\",\n",
    "    \"beir_quora_test\": r\"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\Data Pre-Processing\\beir\\quora\\test\\doc\\docs.json\"\n",
    "}\n",
    "\n",
    "embeddings_base_dir = r\"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\Data Representation\\Bert\"\n",
    "output_base_dir = r\"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\TopicResults\"\n",
    "os.makedirs(output_base_dir, exist_ok=True)\n",
    "\n",
    "for dataset_name, json_path in datasets.items():\n",
    "    if not os.path.exists(json_path):\n",
    "        print(f\"❌ لا يوجد ملف docs.json في {dataset_name} - سيتم تخطيه.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n📂 معالجة الداتا: {dataset_name}\")\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        docs_list = json.load(f)\n",
    "\n",
    "    docs, doc_ids = [], []\n",
    "    for doc in tqdm(docs_list, desc=\"📄 تحميل الوثائق\"):\n",
    "        if \"clean_text\" in doc and doc[\"clean_text\"].strip():\n",
    "            docs.append(doc[\"clean_text\"])\n",
    "            doc_ids.append(doc[\"doc_id\"])\n",
    "\n",
    "    print(f\"✅ Loaded {len(docs)} documents.\")\n",
    "\n",
    "    embedding_path = os.path.join(\n",
    "        embeddings_base_dir,\n",
    "        dataset_name.replace(\"_\", os.sep),\n",
    "        \"doc\",\n",
    "        \"bert_embedding.joblib\"\n",
    "    )\n",
    "\n",
    "    if not os.path.exists(embedding_path):\n",
    "        print(f\"❌ لم يتم العثور على ملف التضمينات في {embedding_path}\")\n",
    "        continue\n",
    "\n",
    "    embedding_data = joblib.load(embedding_path)\n",
    "    if embedding_data[\"doc_ids\"] != doc_ids:\n",
    "        print(f\"❌ ترتيب doc_ids لا يتطابق في {dataset_name}\")\n",
    "        continue\n",
    "\n",
    "    embeddings = np.array(embedding_data[\"embeddings_matrix\"])\n",
    "\n",
    "    print(\"🧠 Fitting BERTopic model...\")\n",
    "    topic_model = BERTopic(language=\"english\", verbose=True)\n",
    "    topics, probs = topic_model.fit_transform(docs, embeddings)\n",
    "\n",
    "    # ============================\n",
    "    # 🔍 إنشاء تمثيل لكل توبيك\n",
    "    # ============================\n",
    "\n",
    "    topic_embeddings = {}\n",
    "    for topic_id in set(topics):\n",
    "        indices = [i for i, t in enumerate(topics) if t == topic_id]\n",
    "        topic_embeds = embeddings[indices]\n",
    "        topic_embeddings[topic_id] = topic_embeds.mean(axis=0)\n",
    "\n",
    "    # 🧪 حفظ كل شيء\n",
    "    output_path = os.path.join(output_base_dir, f\"{dataset_name}_bertopic_results.joblib\")\n",
    "    joblib.dump({\n",
    "        \"topics\": topics,\n",
    "        \"probs\": probs,\n",
    "        \"doc_ids\": doc_ids,\n",
    "        \"model\": topic_model,\n",
    "        \"topic_embeddings\": topic_embeddings  # ⬅️ التمثيلات الجديدة\n",
    "    }, output_path)\n",
    "\n",
    "    print(f\"💾 Results saved to: {output_path}\")\n",
    "\n",
    "    try:\n",
    "        print(\"📊 Visualizing topics...\")\n",
    "        fig = topic_model.visualize_topics()\n",
    "        html_path = os.path.join(output_base_dir, f\"{dataset_name}_topics_visualization.html\")\n",
    "        fig.write_html(html_path)\n",
    "        print(f\"🌐 Visualization saved as HTML: {html_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Visualization failed: {e}\")\n",
    "\n",
    "    print(\"\\n🔎 First 10 document-topic pairs:\")\n",
    "    for i in range(min(10, len(doc_ids))):\n",
    "        print(f\"Doc ID: {doc_ids[i]}  --> Topic: {topics[i]}\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.6 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\Azzam\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\Azzam\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\Azzam\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3116, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3171, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3394, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3639, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Azzam\\AppData\\Local\\Temp\\ipykernel_10648\\3145694699.py\", line 6, in <module>\n",
      "    from bertopic import BERTopic\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\bertopic\\__init__.py\", line 3, in <module>\n",
      "    from bertopic._bertopic import BERTopic\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\bertopic\\_bertopic.py\", line 58, in <module>\n",
      "    from bertopic.backend import BaseEmbedder\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\bertopic\\backend\\__init__.py\", line 22, in <module>\n",
      "    from bertopic.backend._multimodal import MultiModalBackend\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\bertopic\\backend\\_multimodal.py\", line 5, in <module>\n",
      "    from sentence_transformers import SentenceTransformer\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\sentence_transformers\\__init__.py\", line 10, in <module>\n",
      "    from sentence_transformers.backend import (\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\sentence_transformers\\backend.py\", line 11, in <module>\n",
      "    from sentence_transformers.util import disable_datasets_caching, is_datasets_available\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\sentence_transformers\\util.py\", line 20, in <module>\n",
      "    import torch\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\torch\\__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\torch\\functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 معالجة الداتا: antique_train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📄 تحميل الوثائق: 100%|██████████| 403666/403666 [00:00<00:00, 1446849.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 401768 documents.\n",
      "🧠 Fitting BERTopic model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 17:25:55,563 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-07-03 17:33:46,655 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-07-03 17:33:46,682 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-07-03 17:35:08,923 - BERTopic - Cluster - Completed ✓\n",
      "2025-07-03 17:35:09,062 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-07-03 17:35:27,477 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Results saved to: C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\TopicResults\\antique_train_bertopic_results.joblib\n",
      "📊 Visualizing topics...\n",
      "🌐 Visualization saved as HTML: C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\TopicResults\\antique_train_topics_visualization.html\n",
      "\n",
      "🔎 First 10 document-topic pairs:\n",
      "Doc ID: 2020338_0  --> Topic: 1381\n",
      "Doc ID: 2020338_1  --> Topic: 191\n",
      "Doc ID: 2020338_2  --> Topic: -1\n",
      "Doc ID: 2020338_3  --> Topic: -1\n",
      "Doc ID: 2874684_0  --> Topic: 1171\n",
      "Doc ID: 2874684_1  --> Topic: 1317\n",
      "Doc ID: 4193114_0  --> Topic: -1\n",
      "Doc ID: 4193114_1  --> Topic: -1\n",
      "Doc ID: 1908421_0  --> Topic: 1572\n",
      "Doc ID: 1908421_1  --> Topic: 1572\n",
      "\n",
      "📂 معالجة الداتا: beir_quora_test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📄 تحميل الوثائق: 100%|██████████| 522931/522931 [00:00<00:00, 1778696.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 522719 documents.\n",
      "❌ ترتيب doc_ids في ملف التضمين لا يطابق ملف الوثائق في beir_quora_test\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T18:52:10.631360Z",
     "start_time": "2025-07-06T18:12:08.409339Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from bertopic import BERTopic\n",
    "\n",
    "# ============================\n",
    "# 🛠️ المسارات\n",
    "# ============================\n",
    "\n",
    "json_path = r\"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\Data Pre-Processing\\beir\\quora\\test\\doc\\docs.json\"\n",
    "embedding_path = r\"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\Data Representation\\Bert\\beir\\quora\\test\\doc\\bert_embedding.joblib\"\n",
    "output_dir = r\"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\TopicResults\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ============================\n",
    "# 📄 تحميل الوثائق\n",
    "# ============================\n",
    "\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    docs_list = json.load(f)\n",
    "\n",
    "docs = []\n",
    "doc_ids = []\n",
    "for doc in tqdm(docs_list, desc=\"📄 تحميل الوثائق\"):\n",
    "    if \"clean_text\" in doc and doc[\"clean_text\"].strip():\n",
    "        docs.append(doc[\"clean_text\"])\n",
    "        doc_ids.append(doc[\"doc_id\"])\n",
    "\n",
    "print(f\"✅ Loaded {len(docs)} documents.\")\n",
    "\n",
    "# ============================\n",
    "# 🔌 تحميل التضمينات\n",
    "# ============================\n",
    "\n",
    "embedding_data = joblib.load(embedding_path)\n",
    "embedding_doc_ids = embedding_data[\"doc_ids\"]\n",
    "embedding_matrix = embedding_data[\"embeddings_matrix\"]\n",
    "\n",
    "print(\"🔁 إعادة ترتيب التضمينات حسب ترتيب docs.json...\")\n",
    "\n",
    "# 🧠 إنشاء قاموس ID → embedding\n",
    "id_to_embedding = {doc_id: emb for doc_id, emb in zip(embedding_doc_ids, embedding_matrix)}\n",
    "\n",
    "# 🧩 إعادة ترتيب التضمينات\n",
    "reordered_embeddings = []\n",
    "new_docs = []\n",
    "new_doc_ids = []\n",
    "for i, doc_id in enumerate(doc_ids):\n",
    "    if doc_id in id_to_embedding:\n",
    "        reordered_embeddings.append(id_to_embedding[doc_id])\n",
    "        new_docs.append(docs[i])\n",
    "        new_doc_ids.append(doc_id)\n",
    "\n",
    "docs = new_docs\n",
    "doc_ids = new_doc_ids\n",
    "reordered_embeddings = np.array(reordered_embeddings)\n",
    "\n",
    "print(f\"🔢 عدد التضمينات بعد الترتيب: {reordered_embeddings.shape[0]}\")\n",
    "\n",
    "# ============================\n",
    "# 🧠 BERTopic\n",
    "# ============================\n",
    "\n",
    "print(\"🧠 Fitting BERTopic model...\")\n",
    "topic_model = BERTopic(language=\"english\", verbose=True)\n",
    "topics, probs = topic_model.fit_transform(docs, reordered_embeddings)\n",
    "\n",
    "# 🔧 تقليص عدد التوبيكات\n",
    "print(\"🔧 Reducing topics to 4500...\")\n",
    "topic_model.reduce_topics(docs, nr_topics=4500)\n",
    "\n",
    "# ============================\n",
    "# 🎯 توليد تمثيل لكل توبيك\n",
    "# ============================\n",
    "\n",
    "print(\"🧠 Generating topic embeddings...\")\n",
    "topic_embeddings = {}\n",
    "for topic_id in set(topics):\n",
    "    indices = [i for i, t in enumerate(topics) if t == topic_id]\n",
    "    topic_vector = reordered_embeddings[indices].mean(axis=0)\n",
    "    topic_embeddings[topic_id] = topic_vector\n",
    "\n",
    "# ============================\n",
    "# 💾 حفظ النتائج\n",
    "# ============================\n",
    "\n",
    "output_path = os.path.join(output_dir, \"beir_quora_test_bertopic_results.joblib\")\n",
    "joblib.dump({\n",
    "    \"topics\": topics,\n",
    "    \"probs\": probs,\n",
    "    \"doc_ids\": doc_ids,\n",
    "    \"model\": topic_model,\n",
    "    \"topic_embeddings\": topic_embeddings\n",
    "}, output_path)\n",
    "\n",
    "print(f\"💾 Results saved to: {output_path}\")\n",
    "\n",
    "# ============================\n",
    "# 🌐 حفظ التصوّر\n",
    "# ============================\n",
    "\n",
    "try:\n",
    "    print(\"📊 Visualizing topics...\")\n",
    "    fig = topic_model.visualize_topics()\n",
    "    html_path = os.path.join(output_dir, \"beir_quora_test_topics_visualization.html\")\n",
    "    fig.write_html(html_path)\n",
    "    print(f\"🌐 Visualization saved as HTML.\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ فشل في حفظ التصوّر: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n🔎 First 10 document-topic pairs:\")\n",
    "for i in range(min(10, len(doc_ids))):\n",
    "    print(f\"Doc ID: {doc_ids[i]}  --> Topic: {topics[i]}\")"
   ],
   "id": "bf0f764042eb60c6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "📄 تحميل الوثائق: 100%|██████████| 522931/522931 [00:00<00:00, 794727.97it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 522719 documents.\n",
      "🔁 إعادة ترتيب التضمينات حسب ترتيب docs.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 21:13:31,121 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔢 عدد التضمينات بعد الترتيب: 522719\n",
      "🧠 Fitting BERTopic model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 21:30:27,779 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-07-06 21:30:27,864 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-07-06 21:35:06,442 - BERTopic - Cluster - Completed ✓\n",
      "2025-07-06 21:35:06,802 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-07-06 21:35:43,910 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Reducing topics to 4500...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 21:36:59,162 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2025-07-06 21:37:07,121 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-07-06 21:37:48,333 - BERTopic - Representation - Completed ✓\n",
      "2025-07-06 21:37:48,567 - BERTopic - Topic reduction - Reduced number of topics from 8926 to 4500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Generating topic embeddings...\n",
      "💾 Results saved to: C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\TopicResults\\beir_quora_test_bertopic_results.joblib\n",
      "📊 Visualizing topics...\n",
      "🌐 Visualization saved as HTML.\n",
      "\n",
      "🔎 First 10 document-topic pairs:\n",
      "Doc ID: 1  --> Topic: 384\n",
      "Doc ID: 2  --> Topic: 2454\n",
      "Doc ID: 3  --> Topic: -1\n",
      "Doc ID: 4  --> Topic: -1\n",
      "Doc ID: 5  --> Topic: 462\n",
      "Doc ID: 6  --> Topic: -1\n",
      "Doc ID: 7  --> Topic: 1002\n",
      "Doc ID: 8  --> Topic: -1\n",
      "Doc ID: 9  --> Topic: -1\n",
      "Doc ID: 10  --> Topic: -1\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T10:22:16.825386Z",
     "start_time": "2025-07-07T10:21:18.006406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# ============================\n",
    "# 🛠️ إعداد المسارات\n",
    "# ============================\n",
    "input_path = r\"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\TopicResults\\beir_quora_test_bertopic_results.joblib\"\n",
    "output_path = r\"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\TopicResults\\merged_topics_1500.joblib\"\n",
    "TARGET_NUM_TOPICS = 5\n",
    "\n",
    "# ============================\n",
    "# 📦 تحميل التوبيكات\n",
    "# ============================\n",
    "data = joblib.load(input_path)\n",
    "\n",
    "original_topics = data[\"topics\"]  # قائمة: موضوع لكل وثيقة\n",
    "doc_ids = data[\"doc_ids\"]\n",
    "probs = data[\"probs\"]\n",
    "topic_embeddings = data[\"topic_embeddings\"]  # dict\n",
    "model = data.get(\"model\", None)\n",
    "\n",
    "topic_ids = list(topic_embeddings.keys())\n",
    "topic_vectors = np.vstack([topic_embeddings[tid] for tid in topic_ids])\n",
    "\n",
    "# ============================\n",
    "# 🤝 دمج التوبيكات عبر KMeans\n",
    "# ============================\n",
    "print(f\"📉 Clustering {len(topic_ids)} topics → {TARGET_NUM_TOPICS} clusters...\")\n",
    "kmeans = KMeans(n_clusters=TARGET_NUM_TOPICS, random_state=42)\n",
    "new_topic_assignments = kmeans.fit_predict(topic_vectors)\n",
    "\n",
    "# mapping: old_topic_id → new_topic_id\n",
    "old_to_new_topic = {old: int(new) for old, new in zip(topic_ids, new_topic_assignments)}\n",
    "\n",
    "# ============================\n",
    "# 🔁 تعديل التوبيك لكل وثيقة\n",
    "# ============================\n",
    "new_topics = [old_to_new_topic.get(tid, -1) for tid in original_topics]\n",
    "\n",
    "# ============================\n",
    "# 🧠 حساب التمثيل الجديد لكل توبيك\n",
    "# ============================\n",
    "print(\"📐 Recomputing merged topic embeddings...\")\n",
    "new_topic_embeddings = {}\n",
    "for new_topic_id in range(TARGET_NUM_TOPICS):\n",
    "    indices = [i for i, t in enumerate(new_topics) if t == new_topic_id]\n",
    "    if indices:\n",
    "        emb_matrix = np.array([topic_vectors[topic_ids.index(original_topics[i])] for i in indices])\n",
    "        new_topic_embeddings[new_topic_id] = emb_matrix.mean(axis=0)\n",
    "\n",
    "# ============================\n",
    "# 💾 حفظ النتائج\n",
    "# ============================\n",
    "joblib.dump({\n",
    "    \"topics\": new_topics,\n",
    "    \"probs\": probs,\n",
    "    \"doc_ids\": doc_ids,\n",
    "    \"model\": model,\n",
    "    \"topic_embeddings\": new_topic_embeddings\n",
    "}, output_path)\n",
    "\n",
    "print(f\"✅ Merged topics saved to: {output_path}\")\n"
   ],
   "id": "a21b2e17675bfbd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📉 Clustering 8926 topics → 5 clusters...\n",
      "📐 Recomputing merged topic embeddings...\n",
      "✅ Merged topics saved to: C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\TopicResults\\merged_topics_1500.joblib\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T14:13:11.601510Z",
     "start_time": "2025-07-06T14:12:35.322813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import bertopic\n",
    "print(bertopic.__version__)\n"
   ],
   "id": "c6560705203e102b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17.0\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
