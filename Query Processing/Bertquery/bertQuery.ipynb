{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-24T13:08:27.430063Z",
     "start_time": "2025-06-24T13:08:16.748328Z"
    }
   },
   "source": [
    "import ir_datasets\n",
    "import re\n",
    "import joblib\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù„ÙÙ…Ø§Øª ÙˆØ§Ù„Ø£Ø¯ÙˆØ§Øª\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "    return tokens\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "def clean_text(text):\n",
    "    tokens = tokenize(text)\n",
    "    lemmas = lemmatize_tokens(tokens)\n",
    "    return ' '.join(lemmas)\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¯Ø§ØªØ§ Ø³ÙŠØª\n",
    "dataset = ir_datasets.load(\"antique/test/non-offensive\")\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ Ù…ÙˆØ¯ÙŠÙ„ BERT\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def get_bert_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        emb = outputs.last_hidden_state[:, 0, :]\n",
    "    return emb.squeeze().cpu().numpy()\n",
    "\n",
    "# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§ØªØµØ§Ù„ MongoDB\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"ir_project\"]\n",
    "collection = db[\"queries_antique_train\"]\n",
    "\n",
    "# Ù†Ø¸Ù Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§ØªØŒ Ù…Ø«Ù„Ù‡Ø§ØŒ ÙˆØ®Ø²Ù†Ù‡Ø§ ÙÙŠ MongoDB\n",
    "query_docs = []\n",
    "query_ids = []\n",
    "embeddings = []\n",
    "\n",
    "print(\"ğŸ”„ Processing queries...\")\n",
    "\n",
    "for q in tqdm(dataset.queries_iter()):\n",
    "    cleaned = clean_text(q.text)\n",
    "    emb = get_bert_embedding(cleaned)\n",
    "    query_doc = {\n",
    "        \"query_id\": q.query_id,\n",
    "        \"original_text\": q.text,\n",
    "        \"clean_text\": cleaned,\n",
    "        \"bert_embedding\": emb.tolist()  # Ø­ÙˆÙ„ numpy array Ø¥Ù„Ù‰ list Ù„ØªØ®Ø²ÙŠÙ†Ù‡Ø§ ÙÙŠ MongoDB\n",
    "    }\n",
    "    query_docs.append(query_doc)\n",
    "    query_ids.append(q.query_id)\n",
    "    embeddings.append(emb)\n",
    "\n",
    "# Ø­ÙØ¸ ÙÙŠ MongoDB Ø¯ÙØ¹Ø© ÙˆØ§Ø­Ø¯Ø© (bulk insert)\n",
    "if query_docs:\n",
    "    collection.delete_many({})  # ØªÙ†Ø¸ÙŠÙ Ø§Ù„ÙƒÙˆÙ„ÙŠÙƒØ´Ù† Ù‚Ø¨Ù„ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)\n",
    "    collection.insert_many(query_docs)\n",
    "    print(f\"âœ… ØªÙ… ØªØ®Ø²ÙŠÙ† {len(query_docs)} Ø§Ø³ØªØ¹Ù„Ø§Ù… ÙÙŠ MongoDB ÙÙŠ Ø§Ù„ÙƒÙˆÙ„ÙŠÙƒØ´Ù†: {collection.name}\")\n",
    "\n",
    "# Ø­ÙØ¸ ÙÙŠ Ù…Ù„Ù joblib\n",
    "output_dir = r\"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\Query Processing\\Bertquery\\antique\\train\\query_embeddings\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "joblib.dump({\n",
    "    \"query_ids\": query_ids,\n",
    "    \"embeddings\": embeddings,\n",
    "    \"model_name\": MODEL_NAME\n",
    "}, os.path.join(output_dir, \"bert_query_embeddings.joblib\"))\n",
    "\n",
    "print(f\"âœ… ØªÙ… Ø­ÙØ¸ ØªÙ…Ø«ÙŠÙ„Ø§Øª Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª ÙÙŠ {output_dir}\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Azzam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Azzam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Azzam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Processing queries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:02, 66.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ØªÙ… ØªØ®Ø²ÙŠÙ† 176 Ø§Ø³ØªØ¹Ù„Ø§Ù… ÙÙŠ MongoDB ÙÙŠ Ø§Ù„ÙƒÙˆÙ„ÙŠÙƒØ´Ù†: queries_antique_train\n",
      "âœ… ØªÙ… Ø­ÙØ¸ ØªÙ…Ø«ÙŠÙ„Ø§Øª Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª ÙÙŠ C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\Query Processing\\Bertquery\\antique\\train\\query_embeddings\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T09:23:43.864857Z",
     "start_time": "2025-06-24T09:23:43.617867Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "data = joblib.load(r\"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\Query Processing\\Bertquery\\antique\\train\\query_embeddings\\bert_query_embeddings.joblib\")\n",
    "print(len(data[\"query_ids\"]))\n",
    "print(data[\"query_ids\"][:10])\n"
   ],
   "id": "397feb02a0a58578",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2426\n",
      "['3097310', '3910705', '237390', '2247892', '1078492', '782453', '3198658', '1907320', '10895', '992730']\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
