{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-01T13:16:48.601503Z",
     "start_time": "2025-07-01T13:16:47.553901Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ“Š ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 176/176 [00:00<00:00, 15239.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…:\n",
      "Precision@10: 0.1097\n",
      "Recall@10: 0.043\n",
      "MAP@10: 0.0714\n",
      "NDCG@10: 0.1224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 1,
   "source": [
    "import json\n",
    "import ir_datasets\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ qrels Ù…Ù† BEIR Quora\n",
    "dataset = ir_datasets.load(\"antique/test/non-offensive\")\n",
    "qrels = defaultdict(set)\n",
    "for qrel in dataset.qrels_iter():\n",
    "    if int(qrel.relevance) > 0:\n",
    "        qrels[qrel.query_id].add(qrel.doc_id)\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ù…Ù† Ù…Ù„Ù JSON\n",
    "with open(r\"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\Query Matching & Ranking\\TfIdfMatching\\tfidf_results_enhanced_antique.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "# Ø¯ÙˆØ§Ù„ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…\n",
    "def precision_at_k(retrieved, relevant, k):\n",
    "    retrieved_k = retrieved[:k]\n",
    "    if not retrieved_k:\n",
    "        return 0.0\n",
    "    return len([doc for doc in retrieved_k if doc in relevant]) / k\n",
    "\n",
    "def recall_at_k(retrieved, relevant, k):\n",
    "    retrieved_k = retrieved[:k]\n",
    "    if not relevant:\n",
    "        return 0.0\n",
    "    return len([doc for doc in retrieved_k if doc in relevant]) / len(relevant)\n",
    "\n",
    "def average_precision(retrieved, relevant, k):\n",
    "    score = 0.0\n",
    "    hits = 0\n",
    "    for i, doc_id in enumerate(retrieved[:k], start=1):\n",
    "        if doc_id in relevant:\n",
    "            hits += 1\n",
    "            score += hits / i\n",
    "    return score / min(len(relevant), k) if relevant else 0.0\n",
    "\n",
    "def dcg(retrieved, relevant, k):\n",
    "    return sum([(1 if retrieved[i] in relevant else 0) / np.log2(i + 2) for i in range(min(len(retrieved), k))])\n",
    "\n",
    "def idcg(relevant, k):\n",
    "    return sum([1 / np.log2(i + 2) for i in range(min(len(relevant), k))])\n",
    "\n",
    "def ndcg_at_k(retrieved, relevant, k):\n",
    "    dcg_val = dcg(retrieved, relevant, k)\n",
    "    idcg_val = idcg(relevant, k)\n",
    "    return dcg_val / idcg_val if idcg_val > 0 else 0.0\n",
    "\n",
    "# Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª\n",
    "k = 10\n",
    "precisions, recalls, maps, ndcgs = [], [], [], []\n",
    "\n",
    "for qid, retrieved_docs in tqdm(results.items(), desc=\"ðŸ“Š ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª\"):\n",
    "    retrieved_doc_ids = [doc_id for doc_id, _ in retrieved_docs]\n",
    "    relevant_doc_ids = qrels[qid]\n",
    "\n",
    "    precisions.append(precision_at_k(retrieved_doc_ids, relevant_doc_ids, k))\n",
    "    recalls.append(recall_at_k(retrieved_doc_ids, relevant_doc_ids, k))\n",
    "    maps.append(average_precision(retrieved_doc_ids, relevant_doc_ids, k))\n",
    "    ndcgs.append(ndcg_at_k(retrieved_doc_ids, relevant_doc_ids, k))\n",
    "\n",
    "# Ø§Ù„Ù…ØªÙˆØ³Ø·Ø§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©\n",
    "evaluation_results = {\n",
    "    \"Precision@10\": round(np.mean(precisions), 4),\n",
    "    \"Recall@10\": round(np.mean(recalls), 4),\n",
    "    \"MAP@10\": round(np.mean(maps), 4),\n",
    "    \"NDCG@10\": round(np.mean(ndcgs), 4),\n",
    "}\n",
    "\n",
    "print(\"ðŸ“ˆ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…:\")\n",
    "for metric, value in evaluation_results.items():\n",
    "    print(f\"{metric}: {value}\")\n"
   ],
   "id": "initial_id"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T13:53:49.147389Z",
     "start_time": "2025-07-01T13:53:48.896640Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import ir_datasets\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ qrels Ù…Ù† BEIR Quora\n",
    "dataset = ir_datasets.load(\"antique/test/non-offensive\")\n",
    "qrels = defaultdict(set)\n",
    "for qrel in dataset.qrels_iter():\n",
    "    if int(qrel.relevance) > 0:\n",
    "        qrels[qrel.query_id].add(qrel.doc_id)\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ù…Ù† Ù…Ù„Ù JSON\n",
    "with open(r\"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\Query Matching & Ranking\\BertMatching\\bert_results_enhanced_antique.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "# Ø¯ÙˆØ§Ù„ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…\n",
    "def precision_at_k(retrieved, relevant, k):\n",
    "    retrieved_k = retrieved[:k]\n",
    "    if not retrieved_k:\n",
    "        return 0.0\n",
    "    return len([doc for doc in retrieved_k if doc in relevant]) / k\n",
    "\n",
    "def recall_at_k(retrieved, relevant, k):\n",
    "    retrieved_k = retrieved[:k]\n",
    "    if not relevant:\n",
    "        return 0.0\n",
    "    return len([doc for doc in retrieved_k if doc in relevant]) / len(relevant)\n",
    "\n",
    "def average_precision(retrieved, relevant, k):\n",
    "    score = 0.0\n",
    "    hits = 0\n",
    "    for i, doc_id in enumerate(retrieved[:k], start=1):\n",
    "        if doc_id in relevant:\n",
    "            hits += 1\n",
    "            score += hits / i\n",
    "    return score / min(len(relevant), k) if relevant else 0.0\n",
    "\n",
    "def dcg(retrieved, relevant, k):\n",
    "    return sum([(1 if retrieved[i] in relevant else 0) / np.log2(i + 2) for i in range(min(len(retrieved), k))])\n",
    "\n",
    "def idcg(relevant, k):\n",
    "    return sum([1 / np.log2(i + 2) for i in range(min(len(relevant), k))])\n",
    "\n",
    "def ndcg_at_k(retrieved, relevant, k):\n",
    "    dcg_val = dcg(retrieved, relevant, k)\n",
    "    idcg_val = idcg(relevant, k)\n",
    "    return dcg_val / idcg_val if idcg_val > 0 else 0.0\n",
    "\n",
    "# Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª\n",
    "k = 10\n",
    "precisions, recalls, maps, ndcgs = [], [], [], []\n",
    "\n",
    "for qid, retrieved_docs in tqdm(results.items(), desc=\"ðŸ“Š ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª\"):\n",
    "    retrieved_doc_ids = [doc_id for doc_id, _ in retrieved_docs]\n",
    "    relevant_doc_ids = qrels[qid]\n",
    "\n",
    "    precisions.append(precision_at_k(retrieved_doc_ids, relevant_doc_ids, k))\n",
    "    recalls.append(recall_at_k(retrieved_doc_ids, relevant_doc_ids, k))\n",
    "    maps.append(average_precision(retrieved_doc_ids, relevant_doc_ids, k))\n",
    "    ndcgs.append(ndcg_at_k(retrieved_doc_ids, relevant_doc_ids, k))\n",
    "\n",
    "# Ø§Ù„Ù…ØªÙˆØ³Ø·Ø§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©\n",
    "evaluation_results = {\n",
    "    \"Precision@10\": round(np.mean(precisions), 4),\n",
    "    \"Recall@10\": round(np.mean(recalls), 4),\n",
    "    \"MAP@10\": round(np.mean(maps), 4),\n",
    "    \"NDCG@10\": round(np.mean(ndcgs), 4),\n",
    "}\n",
    "\n",
    "print(\"ðŸ“ˆ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…:\")\n",
    "for metric, value in evaluation_results.items():\n",
    "    print(f\"{metric}: {value}\")\n"
   ],
   "id": "151ffb906c3012d5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ“Š ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 176/176 [00:00<00:00, 11456.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…:\n",
      "Precision@10: 0.2227\n",
      "Recall@10: 0.0756\n",
      "MAP@10: 0.1637\n",
      "NDCG@10: 0.256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T14:44:38.750794Z",
     "start_time": "2025-07-02T14:44:35.588461Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Please confirm you agree to the authors' data usage agreement found at <https://ciir.cs.umass.edu/downloads/Antique/readme.txt>\n",
      "[INFO] [starting] https://ciir.cs.umass.edu/downloads/Antique/antique-test.qrel\n",
      "[INFO] [finished] https://ciir.cs.umass.edu/downloads/Antique/antique-test.qrel: [00:00] [150kB] [584kB/s]\n",
      "ðŸ“Š ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 176/176 [00:00<00:00, 9779.65it/s]               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…:\n",
      "Precision@10: 0.2818\n",
      "Recall@10: 0.0976\n",
      "MAP@10: 0.2039\n",
      "NDCG@10: 0.3133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 4,
   "source": [
    "import json\n",
    "import ir_datasets\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ qrels Ù…Ù† BEIR Quora\n",
    "dataset = ir_datasets.load(\"antique/test\")\n",
    "qrels = defaultdict(set)\n",
    "for qrel in dataset.qrels_iter():\n",
    "    if int(qrel.relevance) > 0:\n",
    "        qrels[qrel.query_id].add(qrel.doc_id)\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ù…Ù† Ù…Ù„Ù JSON\n",
    "with open(r\"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\Query Matching & Ranking\\HybridMatching\\hybrid_results_enhanced_antique.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "# Ø¯ÙˆØ§Ù„ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…\n",
    "def precision_at_k(retrieved, relevant, k):\n",
    "    retrieved_k = retrieved[:k]\n",
    "    if not retrieved_k:\n",
    "        return 0.0\n",
    "    return len([doc for doc in retrieved_k if doc in relevant]) / k\n",
    "\n",
    "def recall_at_k(retrieved, relevant, k):\n",
    "    retrieved_k = retrieved[:k]\n",
    "    if not relevant:\n",
    "        return 0.0\n",
    "    return len([doc for doc in retrieved_k if doc in relevant]) / len(relevant)\n",
    "\n",
    "def average_precision(retrieved, relevant, k):\n",
    "    score = 0.0\n",
    "    hits = 0\n",
    "    for i, doc_id in enumerate(retrieved[:k], start=1):\n",
    "        if doc_id in relevant:\n",
    "            hits += 1\n",
    "            score += hits / i\n",
    "    return score / min(len(relevant), k) if relevant else 0.0\n",
    "\n",
    "def dcg(retrieved, relevant, k):\n",
    "    return sum([(1 if retrieved[i] in relevant else 0) / np.log2(i + 2) for i in range(min(len(retrieved), k))])\n",
    "\n",
    "def idcg(relevant, k):\n",
    "    return sum([1 / np.log2(i + 2) for i in range(min(len(relevant), k))])\n",
    "\n",
    "def ndcg_at_k(retrieved, relevant, k):\n",
    "    dcg_val = dcg(retrieved, relevant, k)\n",
    "    idcg_val = idcg(relevant, k)\n",
    "    return dcg_val / idcg_val if idcg_val > 0 else 0.0\n",
    "\n",
    "# Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª\n",
    "k = 10\n",
    "precisions, recalls, maps, ndcgs = [], [], [], []\n",
    "\n",
    "for qid, retrieved_docs in tqdm(results.items(), desc=\"ðŸ“Š ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª\"):\n",
    "    retrieved_doc_ids = [doc_id for doc_id, _ in retrieved_docs]\n",
    "    relevant_doc_ids = qrels[qid]\n",
    "\n",
    "    precisions.append(precision_at_k(retrieved_doc_ids, relevant_doc_ids, k))\n",
    "    recalls.append(recall_at_k(retrieved_doc_ids, relevant_doc_ids, k))\n",
    "    maps.append(average_precision(retrieved_doc_ids, relevant_doc_ids, k))\n",
    "    ndcgs.append(ndcg_at_k(retrieved_doc_ids, relevant_doc_ids, k))\n",
    "\n",
    "# Ø§Ù„Ù…ØªÙˆØ³Ø·Ø§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©\n",
    "evaluation_results = {\n",
    "    \"Precision@10\": round(np.mean(precisions), 4),\n",
    "    \"Recall@10\": round(np.mean(recalls), 4),\n",
    "    \"MAP@10\": round(np.mean(maps), 4),\n",
    "    \"NDCG@10\": round(np.mean(ndcgs), 4),\n",
    "}\n",
    "\n",
    "print(\"ðŸ“ˆ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…:\")\n",
    "for metric, value in evaluation_results.items():\n",
    "    print(f\"{metric}: {value}\")\n"
   ],
   "id": "44b280834817c44f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T09:43:15.070742Z",
     "start_time": "2025-07-03T09:43:14.827519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import ir_datasets\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ qrels Ù…Ù† BEIR Quora\n",
    "dataset = ir_datasets.load(\"beir/quora/test\")\n",
    "qrels = defaultdict(set)\n",
    "for qrel in dataset.qrels_iter():\n",
    "    if int(qrel.relevance) > 0:\n",
    "        qrels[qrel.query_id].add(qrel.doc_id)\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ù…Ù† Ù…Ù„Ù JSON\n",
    "with open(r\"C:\\Users\\Azzam\\PycharmProjects\\PythonProject\\Query Matching & Ranking\\TfIdfMatching\\tfidf_results_enhanced_qoura.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "# Ø¯ÙˆØ§Ù„ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…\n",
    "def precision_at_k(retrieved, relevant, k):\n",
    "    retrieved_k = retrieved[:k]\n",
    "    if not retrieved_k:\n",
    "        return 0.0\n",
    "    return len([doc for doc in retrieved_k if doc in relevant]) / k\n",
    "\n",
    "def recall_at_k(retrieved, relevant, k):\n",
    "    retrieved_k = retrieved[:k]\n",
    "    if not relevant:\n",
    "        return 0.0\n",
    "    return len([doc for doc in retrieved_k if doc in relevant]) / len(relevant)\n",
    "\n",
    "def average_precision(retrieved, relevant, k):\n",
    "    score = 0.0\n",
    "    hits = 0\n",
    "    for i, doc_id in enumerate(retrieved[:k], start=1):\n",
    "        if doc_id in relevant:\n",
    "            hits += 1\n",
    "            score += hits / i\n",
    "    return score / min(len(relevant), k) if relevant else 0.0\n",
    "\n",
    "def dcg(retrieved, relevant, k):\n",
    "    return sum([(1 if retrieved[i] in relevant else 0) / np.log2(i + 2) for i in range(min(len(retrieved), k))])\n",
    "\n",
    "def idcg(relevant, k):\n",
    "    return sum([1 / np.log2(i + 2) for i in range(min(len(relevant), k))])\n",
    "\n",
    "def ndcg_at_k(retrieved, relevant, k):\n",
    "    dcg_val = dcg(retrieved, relevant, k)\n",
    "    idcg_val = idcg(relevant, k)\n",
    "    return dcg_val / idcg_val if idcg_val > 0 else 0.0\n",
    "\n",
    "# Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª\n",
    "k = 10\n",
    "precisions, recalls, maps, ndcgs = [], [], [], []\n",
    "\n",
    "for qid, retrieved_docs in tqdm(results.items(), desc=\"ðŸ“Š ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª\"):\n",
    "    retrieved_doc_ids = [doc_id for doc_id, _ in retrieved_docs]\n",
    "    relevant_doc_ids = qrels[qid]\n",
    "\n",
    "    precisions.append(precision_at_k(retrieved_doc_ids, relevant_doc_ids, k))\n",
    "    recalls.append(recall_at_k(retrieved_doc_ids, relevant_doc_ids, k))\n",
    "    maps.append(average_precision(retrieved_doc_ids, relevant_doc_ids, k))\n",
    "    ndcgs.append(ndcg_at_k(retrieved_doc_ids, relevant_doc_ids, k))\n",
    "\n",
    "# Ø§Ù„Ù…ØªÙˆØ³Ø·Ø§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©\n",
    "evaluation_results = {\n",
    "    \"Precision@10\": round(np.mean(precisions), 4),\n",
    "    \"Recall@10\": round(np.mean(recalls), 4),\n",
    "    \"MAP@10\": round(np.mean(maps), 4),\n",
    "    \"NDCG@10\": round(np.mean(ndcgs), 4),\n",
    "}\n",
    "\n",
    "print(\"ðŸ“ˆ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…:\")\n",
    "for metric, value in evaluation_results.items():\n",
    "    print(f\"{metric}: {value}\")\n"
   ],
   "id": "4870fc3ef43919d7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ“Š ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 33341.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…:\n",
      "Precision@10: 0.027\n",
      "Recall@10: 0.1826\n",
      "MAP@10: 0.1186\n",
      "NDCG@10: 0.1389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
